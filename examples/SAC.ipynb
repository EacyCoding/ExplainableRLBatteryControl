{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e5fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 00:04:03.033670: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-26 00:04:03.047473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756159443.063255 3645259 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756159443.067629 3645259 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756159443.079371 3645259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756159443.079384 3645259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756159443.079385 3645259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756159443.079387 3645259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-26 00:04:03.083832: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.7\n",
      "Torch : 2.7.0+cu126\n",
      "CityLearn: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Basic imports & versions\n",
    "import sys, os, json, time, math, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "\n",
    "# SB3\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Utils and CityLearn\n",
    "from utils.env_utils import keep_only_electrical_storage, keep_only_core_observations, PPOTrainLogger\n",
    "import citylearn\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.wrappers import NormalizedObservationWrapper, StableBaselines3Wrapper\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Torch :', th.__version__)\n",
    "print('CityLearn:', citylearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9251844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (consistent with PPO/A2C)\n",
    "DATASET_NAME = 'citylearn_challenge_2023_phase_3_1'\n",
    "ROOT_DIR = r'/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "REWARD_FN = {  # CostReward\n",
    "    'type': 'citylearn.reward_function.CostReward',\n",
    "    'attributes': {}\n",
    "}\n",
    "PRICING_FILE = 'pricing_germany_2023_june_to_august.csv'\n",
    "ONLY_ELECTRICAL_STORAGE = True\n",
    "EVAL_EPISODES = 10\n",
    "\n",
    "# Action label helpers (for comparability with RBC/DQN plots)\n",
    "ACTION_LABELS = ['strong_discharge','mild_discharge','idle','mild_charge','strong_charge']\n",
    "NAME_TO_FRAC = {\n",
    "    'strong_discharge': -1.0,\n",
    "    'mild_discharge': -0.5,\n",
    "    'idle': 0.0,\n",
    "    'mild_charge': 0.5,\n",
    "    'strong_charge': 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b020bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Go here /home/iai/cj9272/.cache/citylearn/v2.4.1/datasets/citylearn_challenge_2023_phase_3_1/schema.json \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1\n",
      "Dataset '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1' copied to '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1/../../../../results/2025-08-26_00-04-06'\n",
      "Initial time step: 0\n",
      "Number of time steps: 2208\n",
      "Central agent: True\n",
      "Number of buildings: 6\n",
      "Electrical storage capacity: {'Building_1': 4.0, 'Building_2': 4.0, 'Building_3': 3.3, 'Building_4': 3.3, 'Building_5': 4.0, 'Building_6': 3.3}\n",
      "Action_space: [Box(-1.0, 1.0, (6,), float32)]\n",
      "Number of Observations: 20\n"
     ]
    }
   ],
   "source": [
    "# --- Load schema ---\n",
    "dataset = DataSet()\n",
    "schema = dataset.get_schema(DATASET_NAME)\n",
    "schema['root_directory'] = ROOT_DIR\n",
    "schema['reward_function'] = REWARD_FN\n",
    "\n",
    "# Set pricing file on all buildings\n",
    "price_file = PRICING_FILE\n",
    "if 'buildings' not in schema:\n",
    "    raise RuntimeError(\"schema does not contain 'buildings'\")\n",
    "for bname, bconf in schema['buildings'].items():\n",
    "    bconf['pricing'] = price_file\n",
    "\n",
    "# Keep only electrical storage and core obs (same as other notebooks)\n",
    "schema = keep_only_electrical_storage(schema)\n",
    "schema = keep_only_core_observations(schema, extra_keep=['carbon_intensity'], drop_predictions=False)\n",
    "\n",
    "# Create base env\n",
    "env = CityLearnEnv(schema, central_agent=True)\n",
    "\n",
    "# Basic checks\n",
    "print('Initial time step:', env.time_step)\n",
    "print('Number of time steps:', env.time_steps)\n",
    "print('Central agent:', env.central_agent)\n",
    "print('Number of buildings:', len(env.buildings))\n",
    "print('Electrical storage capacity:', {b.name: b.electrical_storage.capacity for b in env.buildings})\n",
    "print('Action_space:', env.action_space)\n",
    "print('Number of Observations:', len(env.observations[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2acbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Total time elapsed for 88320 steps: 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "# Build & Train SAC (continuous actions; no discretization needed)\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', 'sac')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "train_env = NormalizedObservationWrapper(env)\n",
    "train_env = StableBaselines3Wrapper(train_env)\n",
    "train_env = Monitor(train_env, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "\n",
    "logger = PPOTrainLogger()  # generic callback capturing ep/step stats\n",
    "\n",
    "TOTAL_TIMESTEPS = 2208 * 40\n",
    "\n",
    "model = SAC(\n",
    "    policy='MlpPolicy',\n",
    "    env=train_env,\n",
    "    seed=0,\n",
    "    device='cpu',\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=200_000,\n",
    "    batch_size=256,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    "    ent_coef='auto',\n",
    "    train_freq=(1, 'step'),\n",
    "    gradient_steps=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256], activation_fn=th.nn.ReLU),\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=logger)\n",
    "print(f\"Total time elapsed for {TOTAL_TIMESTEPS} steps: {(time.time()-t0):.2f} seconds\")\n",
    "model.save(\"sac_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c020c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1\n",
      "Dataset '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1' copied to '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1/../../../../results/2025-08-26_00-04-09'\n",
      "[Eval] Ep 1/10 return=-1719.148 len=2207 mean=-0.779 time=282.98s speed=7.8 steps/s\n",
      "[Eval] Ep 2/10 return=-1720.526 len=2207 mean=-0.780 time=271.55s speed=8.1 steps/s\n",
      "[Eval] Ep 3/10 return=-1731.370 len=2207 mean=-0.784 time=301.77s speed=7.3 steps/s\n",
      "[Eval] Ep 4/10 return=-1723.673 len=2207 mean=-0.781 time=301.89s speed=7.3 steps/s\n",
      "[Eval] Ep 5/10 return=-1734.389 len=2207 mean=-0.786 time=301.85s speed=7.3 steps/s\n",
      "[Eval] Ep 6/10 return=-1724.375 len=2207 mean=-0.781 time=295.89s speed=7.5 steps/s\n",
      "[Eval] Ep 7/10 return=-1723.442 len=2207 mean=-0.781 time=157.46s speed=14.0 steps/s\n",
      "[Eval] Ep 8/10 return=-1727.508 len=2207 mean=-0.783 time=157.50s speed=14.0 steps/s\n",
      "[Eval] Ep 9/10 return=-1727.934 len=2207 mean=-0.783 time=156.83s speed=14.1 steps/s\n",
      "[Eval] Ep 10/10 return=-1722.633 len=2207 mean=-0.781 time=157.34s speed=14.0 steps/s\n",
      "Total eval time: 2385.50s  | Avg/ep: 238.55s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>cumulative reward</th>\n",
       "      <th>length</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>min_step_reward</th>\n",
       "      <th>max_step_reward</th>\n",
       "      <th>std_step_reward</th>\n",
       "      <th>steps_per_sec</th>\n",
       "      <th>wall_time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1719.147636</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.778952</td>\n",
       "      <td>-4.859519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655825</td>\n",
       "      <td>7.799071</td>\n",
       "      <td>282.982430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1720.525804</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.779577</td>\n",
       "      <td>-4.858914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649378</td>\n",
       "      <td>8.127477</td>\n",
       "      <td>271.547986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1731.369847</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.784490</td>\n",
       "      <td>-5.596459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665878</td>\n",
       "      <td>7.313416</td>\n",
       "      <td>301.774145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1723.673095</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.781003</td>\n",
       "      <td>-4.579178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649517</td>\n",
       "      <td>7.310537</td>\n",
       "      <td>301.893019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1734.388899</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.785858</td>\n",
       "      <td>-5.200827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667839</td>\n",
       "      <td>7.311602</td>\n",
       "      <td>301.849016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-1724.374941</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.781321</td>\n",
       "      <td>-4.449775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655506</td>\n",
       "      <td>7.458736</td>\n",
       "      <td>295.894647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-1723.441887</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.780898</td>\n",
       "      <td>-4.893862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.651756</td>\n",
       "      <td>14.016424</td>\n",
       "      <td>157.458134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-1727.507542</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.782740</td>\n",
       "      <td>-4.590384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660531</td>\n",
       "      <td>14.012317</td>\n",
       "      <td>157.504283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-1727.933981</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.782933</td>\n",
       "      <td>-5.164514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660553</td>\n",
       "      <td>14.072130</td>\n",
       "      <td>156.834818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-1722.632911</td>\n",
       "      <td>2207</td>\n",
       "      <td>-0.780531</td>\n",
       "      <td>-5.442038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665439</td>\n",
       "      <td>14.027390</td>\n",
       "      <td>157.335042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  cumulative reward  length  mean_reward  min_step_reward  \\\n",
       "0        1       -1719.147636    2207    -0.778952        -4.859519   \n",
       "1        2       -1720.525804    2207    -0.779577        -4.858914   \n",
       "2        3       -1731.369847    2207    -0.784490        -5.596459   \n",
       "3        4       -1723.673095    2207    -0.781003        -4.579178   \n",
       "4        5       -1734.388899    2207    -0.785858        -5.200827   \n",
       "5        6       -1724.374941    2207    -0.781321        -4.449775   \n",
       "6        7       -1723.441887    2207    -0.780898        -4.893862   \n",
       "7        8       -1727.507542    2207    -0.782740        -4.590384   \n",
       "8        9       -1727.933981    2207    -0.782933        -5.164514   \n",
       "9       10       -1722.632911    2207    -0.780531        -5.442038   \n",
       "\n",
       "   max_step_reward  std_step_reward  steps_per_sec  wall_time_s  \n",
       "0              0.0         0.655825       7.799071   282.982430  \n",
       "1              0.0         0.649378       8.127477   271.547986  \n",
       "2              0.0         0.665878       7.313416   301.774145  \n",
       "3              0.0         0.649517       7.310537   301.893019  \n",
       "4              0.0         0.667839       7.311602   301.849016  \n",
       "5              0.0         0.655506       7.458736   295.894647  \n",
       "6              0.0         0.651756      14.016424   157.458134  \n",
       "7              0.0         0.660531      14.012317   157.504283  \n",
       "8              0.0         0.660553      14.072130   156.834818  \n",
       "9              0.0         0.665439      14.027390   157.335042  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eval env (same wrappers)\n",
    "eval_env = CityLearnEnv(schema, central_agent=True)\n",
    "eval_env = NormalizedObservationWrapper(eval_env)\n",
    "eval_env = StableBaselines3Wrapper(eval_env)\n",
    "\n",
    "# Optionally reload (or use in-memory model)\n",
    "# model = SAC.load(\"sac_01\")\n",
    "\n",
    "# Enhanced evaluation: collect actions and rewards, print KPIs and timings.\n",
    "def evaluate_with_metrics(model, env, episodes=5, deterministic=True, render=False):\n",
    "    ep_metrics, all_step_rewards, all_kpis, all_actions = [], [], [], []\n",
    "    t_global_start = time.time()\n",
    "    for ep in range(1, episodes+1):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret, ep_len = 0.0, 0\n",
    "        step_rewards = []\n",
    "        action_list = []\n",
    "        t_ep_start = time.time()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "            action_list.append(np.array(action))\n",
    "            obs, r, terminated, truncated, info = env.step(action)\n",
    "            done = bool(terminated or truncated)\n",
    "            ep_ret += float(r)\n",
    "            step_rewards.append(float(r))\n",
    "            ep_len += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "        kpis = env.unwrapped.evaluate()\n",
    "        all_kpis.append(kpis)\n",
    "        all_actions.append(np.vstack(action_list))\n",
    "        t_ep = time.time() - t_ep_start\n",
    "        steps_per_sec = ep_len / max(t_ep, 1e-9)\n",
    "        ep_metrics.append({\n",
    "            'episode': ep, 'cumulative reward': ep_ret, 'length': ep_len,\n",
    "            'mean_reward': ep_ret/ep_len if ep_len else np.nan,\n",
    "            'min_step_reward': float(np.min(step_rewards)),\n",
    "            'max_step_reward': float(np.max(step_rewards)),\n",
    "            'std_step_reward': float(np.std(step_rewards)),\n",
    "            'steps_per_sec': steps_per_sec, 'wall_time_s': t_ep\n",
    "        })\n",
    "        all_step_rewards.extend(step_rewards)\n",
    "        print(f\"[Eval] Ep {ep}/{episodes} return={ep_ret:.3f} len={ep_len} \"\n",
    "              f\"mean={ep_ret/ep_len:.3f} time={t_ep:.2f}s speed={steps_per_sec:.1f} steps/s\")\n",
    "        # print(\"KPIs:\", kpis)\n",
    "    print(f\"Total eval time: {time.time()-t_global_start:.2f}s  | Avg/ep: {(time.time()-t_global_start)/episodes:.2f}s\")\n",
    "    metrics_df = pd.DataFrame(ep_metrics)\n",
    "    display(metrics_df)\n",
    "    return metrics_df, np.array(all_step_rewards), all_kpis, all_actions\n",
    "\n",
    "# Run evaluation\n",
    "metrics_stoch, step_rewards_stoch, kpis_stoch, actions_stoch = evaluate_with_metrics(\n",
    "    model, eval_env, episodes=EVAL_EPISODES, deterministic=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363dc967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No episode returns in logger.ep_df\n"
     ]
    }
   ],
   "source": [
    "# Quick learning check\n",
    "if hasattr(logger, 'ep_df') and len(logger.ep_df):\n",
    "    ep_df = logger.ep_df\n",
    "    first_n, last_n = 10, 10\n",
    "    first_mean = ep_df['return'].head(first_n).mean()\n",
    "    last_mean  = ep_df['return'].tail(last_n).mean()\n",
    "    print(f'First {first_n} ep mean return: {first_mean:.3f}')\n",
    "    print(f'Last  {last_n} ep mean return: {last_mean:.3f}')\n",
    "    print('Learning indicator (last - first):', f'{(last_mean-first_mean):.3f}')\n",
    "else:\n",
    "    print('No episode returns in logger.ep_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d06b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Choose a building to visualize\u001b[39;00m\n\u001b[32m      7\u001b[39m building_name = \u001b[33m'\u001b[39m\u001b[33mBuilding_1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m bld_names = [b.name \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[43meval_env\u001b[49m.unwrapped.buildings]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m building_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m bld_names:\n\u001b[32m     10\u001b[39m     building_name = bld_names[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'eval_env' is not defined"
     ]
    }
   ],
   "source": [
    "# Plots (same as PPO/A2C; quantize actions to 5 bins for comparability)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a building to visualize\n",
    "building_name = 'Building_1'\n",
    "bld_names = [b.name for b in eval_env.unwrapped.buildings]\n",
    "if building_name not in bld_names:\n",
    "    building_name = bld_names[0]\n",
    "b_idx = bld_names.index(building_name)\n",
    "print(f'Plotting actions for building: {building_name} (index {b_idx})')\n",
    "\n",
    "# Flatten evaluated actions across episodes -> shape (total_steps, n_actions)\n",
    "actions_arr = np.concatenate(actions_stoch, axis=0)  # each item shape (ep_len, n_actions)\n",
    "n_actions = actions_arr.shape[1]\n",
    "total_steps = actions_arr.shape[0]\n",
    "\n",
    "# Build episode boundaries from metrics\n",
    "ep_lengths = metrics_stoch['length'].to_numpy()\n",
    "ep_offsets = np.cumsum(np.concatenate([[0], ep_lengths[:-1]]))\n",
    "hour_index = np.arange(total_steps)\n",
    "\n",
    "# Load pricing for the chosen building\n",
    "price_file = schema['buildings'][building_name]['pricing']\n",
    "price_path = os.path.join(ROOT_DIR, price_file)\n",
    "prc = pd.read_csv(price_path)\n",
    "T = len(prc)  # usually 2208\n",
    "\n",
    "# Map continuous action to 5 RBC-like labels by nearest of [-1,-0.5,0,0.5,1]\n",
    "bin_centers = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "ACTION_LABELS = ['strong_discharge','mild_discharge','idle','mild_charge','strong_charge']\n",
    "def cont_to_label(vals):\n",
    "    idx = np.argmin(np.abs(vals[..., None] - bin_centers[None, ...]), axis=-1)\n",
    "    return idx  # 0..4\n",
    "\n",
    "# ----- Plot 1: Actions over Time (for selected building) -----\n",
    "window = 500\n",
    "act_series = actions_arr[:, b_idx]\n",
    "act_ma = pd.Series(act_series).rolling(window, min_periods=1).mean().to_numpy()\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.scatter(hour_index, act_series, s=5, alpha=0.35, label='raw')\n",
    "plt.plot(hour_index, act_ma, color='C2', lw=2, label=f'{window}-step MA')\n",
    "plt.title(f'Actions over time (raw + MA) - {building_name}')\n",
    "plt.xlabel('Step'); plt.ylabel('Action [-1..1]'); plt.grid(True); plt.legend(loc='upper right')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----- Plot 2: Action Distribution (quantized to 5 bins like RBC) -----\n",
    "labels_idx = cont_to_label(act_series)\n",
    "counts = pd.Series(labels_idx).value_counts().reindex(range(5)).fillna(0).astype(int)\n",
    "plt.figure(figsize=(4,3))\n",
    "ax = counts.plot(kind='bar', color=['C0','C1','C2','C3','C4'])\n",
    "ax.set_xticklabels(ACTION_LABELS, rotation=45, ha='right')\n",
    "total = counts.sum()\n",
    "ax.bar_label(ax.containers[0], labels=[f'{int(v)}\\n({v/total:.1%})' for v in counts.values])\n",
    "ax.set_ylim(0, counts.max()*1.5)\n",
    "plt.ylabel('Count'); plt.title('Action Distribution (5-bin)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----- Plot 3: Episode 1 Reward MA vs Electricity Price -----\n",
    "step_rewards_all = step_rewards_stoch  # flattened across eval episodes\n",
    "ep1_len = int(ep_lengths[0])\n",
    "ep1_rewards = step_rewards_all[:ep1_len]\n",
    "window_r = 200\n",
    "ep1_reward_ma = pd.Series(ep1_rewards).rolling(window_r, min_periods=1).mean().to_numpy()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7,3))\n",
    "ax1.plot(np.arange(ep1_len), ep1_reward_ma, color='C3', label=f'Reward MA (w={window_r})')\n",
    "ax1.set_ylabel(f'MA Reward ({window_r})', color='C3'); ax1.tick_params(axis='y', labelcolor='C3')\n",
    "ax1.set_xlabel('Hour (episode 1)'); ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(prc.index[:ep1_len], prc['electricity_pricing'][:ep1_len], color='C0', alpha=0.5, label='Electricity Price')\n",
    "ax2.set_ylabel('Electricity Price', color='C0'); ax2.tick_params(axis='y', labelcolor='C0')\n",
    "\n",
    "lines = ax1.get_lines() + ax2.get_lines()\n",
    "ax1.legend(lines, [l.get_label() for l in lines], loc='lower left')\n",
    "plt.title(f'Episode 1: Reward MA vs Electricity Price - {building_name}')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----- Plot 4: Full-horizon Reward vs Electricity Price (tiled across episodes) -----\n",
    "window_reward = 500\n",
    "reward_ma = pd.Series(step_rewards_all).rolling(window_reward, min_periods=1).mean().to_numpy()\n",
    "n_hours_total = len(step_rewards_all)\n",
    "n_repeats = int(np.ceil(n_hours_total / T))\n",
    "price_tiled = np.tile(prc['electricity_pricing'].values, n_repeats)[:n_hours_total]\n",
    "price_ma = pd.Series(price_tiled).rolling(24, min_periods=1).mean().to_numpy()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,4))\n",
    "ax1.plot(np.arange(n_hours_total), reward_ma, color='C3', label=f'Reward MA (w={window_reward})')\n",
    "ax1.set_xlabel('Step'); ax1.set_ylabel('Reward (MA)', color='C3'); ax1.tick_params(axis='y', labelcolor='C3'); ax1.grid(True, axis='x', alpha=0.4)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.arange(n_hours_total), price_ma, color='C0', lw=2, label='Price 24h MA')\n",
    "ax2.set_ylabel('Electricity Price', color='C0'); ax2.tick_params(axis='y', labelcolor='C0')\n",
    "\n",
    "# Mark episode boundaries\n",
    "for off in ep_offsets[1:]:\n",
    "    ax1.axvline(off, color='k', linestyle='--', alpha=0.15, linewidth=0.8)\n",
    "\n",
    "lines = ax1.get_lines() + ax2.get_lines()\n",
    "ax1.legend(lines, [l.get_label() for l in lines], loc='upper right', frameon=False)\n",
    "plt.title('Eval Reward vs Electricity Pricing (tiled)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----- Plot 5: Solar Generation vs Reward MA Across Eval Episodes -----\n",
    "bld_file = schema['buildings'][building_name]['energy_simulation']\n",
    "bld_path = os.path.join(ROOT_DIR, bld_file)\n",
    "bld_df = pd.read_csv(bld_path)\n",
    "solar_vals = bld_df['solar_generation'].values\n",
    "\n",
    "solar_tiled = np.tile(solar_vals, int(np.ceil(n_hours_total / T)))[:n_hours_total]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,4))\n",
    "ax1.plot(np.arange(n_hours_total), reward_ma, color='C3', label=f'Reward MA (w={window_reward})')\n",
    "ax1.set_ylabel('Reward (MA)', color='C3'); ax1.tick_params(axis='y', labelcolor='C3'); ax1.grid(True, axis='x', alpha=0.3)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.arange(n_hours_total), solar_tiled, color='C2', alpha=0.75, label='Solar Generation')\n",
    "ax2.set_ylabel('Solar Generation', color='C2'); ax2.tick_params(axis='y', labelcolor='C2')\n",
    "\n",
    "lines = ax1.get_lines() + ax2.get_lines()\n",
    "ax1.legend(lines, [l.get_label() for l in lines], loc='upper right', frameon=False)\n",
    "plt.title(f'Solar Generation vs Reward MA Across {len(ep_lengths)} Eval Episodes - {building_name}')\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (citylearn_env)",
   "language": "python",
   "name": "citylearn_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
