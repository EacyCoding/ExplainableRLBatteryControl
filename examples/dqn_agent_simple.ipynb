{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0beca837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b112d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%capture\\n\\n# The environment we will be working with\\n%pip install CityLearn==2.1.2\\n\\n# For participant interactions (buttons)\\n%pip install ipywidgets\\n\\n# To generate static figures\\n%pip install matplotlib\\n%pip install seaborn\\n\\n# Provide standard RL algorithms\\n%pip install stable-baselines3\\n\\n# Enable gym compatibility with later stable-baselines3 versions\\n%pip install shimmy\\n\\n# Results submission\\n%pip install requests\\n%pip install beautifulsoup4\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%capture\n",
    "\n",
    "# The environment we will be working with\n",
    "%pip install CityLearn==2.1.2\n",
    "\n",
    "# For participant interactions (buttons)\n",
    "%pip install ipywidgets\n",
    "\n",
    "# To generate static figures\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "\n",
    "# Provide standard RL algorithms\n",
    "%pip install stable-baselines3\n",
    "\n",
    "# Enable gym compatibility with later stable-baselines3 versions\n",
    "%pip install shimmy\n",
    "\n",
    "# Results submission\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81f853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /hkfs/home/haicore/iai/cj9272/citylearn_env/bin/python\n",
      "Pip:    /software/all/jupyter/ai/2025-05-23/bin/pip\n",
      "\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 17:44:14.848412: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-18 17:44:14.862117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755531854.875662  808176 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755531854.879992  808176 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755531854.891630  808176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755531854.891645  808176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755531854.891646  808176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755531854.891648  808176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-18 17:44:14.896249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Pip:   \", subprocess.run([\"which\",\"pip\"], capture_output=True, text=True).stdout)\n",
    "\n",
    "# System operations\n",
    "import os\n",
    "\n",
    "# Type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "\n",
    "# CityLearn\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction, SolarPenaltyReward\n",
    "\n",
    "# Baseline RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "# set all plotted figures without margins\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b053b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Go here /home/iai/cj9272/.cache/citylearn/v2.4.1/datasets/citylearn_challenge_2023_phase_3_1/schema.json \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'hour', 'day_type', 'daylight_savings_status', 'indoor_dry_bulb_temperature', 'average_unmet_cooling_setpoint_difference', 'indoor_relative_humidity', 'non_shiftable_load', 'dhw_demand', 'cooling_demand', 'heating_demand', 'solar_generation', 'occupant_count', 'indoor_dry_bulb_temperature_cooling_set_point', 'indoor_dry_bulb_temperature_heating_set_point', 'hvac_mode']\n"
     ]
    }
   ],
   "source": [
    "# --- keep only electrical storage so we have 1D continuous action to discretize ---\n",
    "def keep_only_electrical_storage(schema: dict) -> dict:\n",
    "    if 'actions' in schema:\n",
    "        for a in list(schema['actions'].keys()):\n",
    "            schema['actions'][a]['active'] = (a == 'electrical_storage')\n",
    "    if 'buildings' in schema:\n",
    "        for b in schema['buildings']:\n",
    "            if 'actions' in b:\n",
    "                for a in list(b['actions'].keys()):\n",
    "                    b['actions'][a]['active'] = (a == 'electrical_storage')\n",
    "    return schema\n",
    "# Dataset\n",
    "DATASET_NAME = 'citylearn_challenge_2023_phase_3_1'\n",
    "schema = DataSet().get_schema(DATASET_NAME)\n",
    "schema['root_directory'] = r'/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "schema = keep_only_electrical_storage(schema) # Activate only the electrical storage control (fix \"Expected 18 actions but got 1\")\n",
    "# Set reward function\n",
    "schema['reward_function'] = { # CostReward Function\n",
    "    'type': 'citylearn.reward_function.CostReward',\n",
    "    'attributes': {}\n",
    "}\n",
    "# Set pricing file\n",
    "price_file = 'pricing_germany_2023_june_to_august.csv'  # Pricing CSV\n",
    "if 'buildings' not in schema:\n",
    "    raise RuntimeError(\"schema does not contain 'buildings' (make sure schema is loaded first)\")\n",
    "for bname, bconf in schema['buildings'].items():\n",
    "    bconf['pricing'] = price_file\n",
    "\n",
    "# Building\n",
    "#root_directory = schema['root_directory']\n",
    "root_directory = 'Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "building_name = 'Building_1'\n",
    "# Weather data\n",
    "filename = schema['buildings'][building_name]['weather']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "weather_data = pd.read_csv(filepath)\n",
    "# Pricing data (simple)\n",
    "filename = schema['buildings'][building_name]['pricing']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "pricing_data = pd.read_csv(filepath)\n",
    "# Carbon Intensity data\n",
    "filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "carbon_intensity_data = pd.read_csv(filepath)\n",
    "# building data\n",
    "filename = schema['buildings'][building_name]['energy_simulation']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "building_data = pd.read_csv(filepath)\n",
    "\n",
    "# Display building data\n",
    "# display(building_data.head())\n",
    "# display(building_data.describe(include='all'))\n",
    "\n",
    "bld = building_data.copy()\n",
    "wth = weather_data.copy()\n",
    "prc = pricing_data.copy()\n",
    "car = carbon_intensity_data.copy()\n",
    "\n",
    "print(building_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcff422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [\n",
    "    # building_df\n",
    "    'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "    'indoor_dry_bulb_temperature',\n",
    "    'average_unmet_cooling_setpoint_difference',\n",
    "    'indoor_relative_humidity',\n",
    "    'non_shiftable_load', 'dhw_demand',\n",
    "    'cooling_demand', 'heating_demand',\n",
    "    'solar_generation', 'occupant_count',\n",
    "    'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "    'indoor_dry_bulb_temperature_heating_set_point', 'hvac_mode',\n",
    "    # weather_df\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance',\n",
    "    'outdoor_dry_bulb_temperature_predicted_1',\n",
    "    'outdoor_dry_bulb_temperature_predicted_2',\n",
    "    'outdoor_dry_bulb_temperature_predicted_3',\n",
    "    'outdoor_relative_humidity_predicted_1',\n",
    "    'outdoor_relative_humidity_predicted_2',\n",
    "    'outdoor_relative_humidity_predicted_3',\n",
    "    'diffuse_solar_irradiance_predicted_1',\n",
    "    'diffuse_solar_irradiance_predicted_2',\n",
    "    'diffuse_solar_irradiance_predicted_3',\n",
    "    'direct_solar_irradiance_predicted_1',\n",
    "    'direct_solar_irradiance_predicted_2',\n",
    "    'direct_solar_irradiance_predicted_3',\n",
    "    # carbon_df \n",
    "    'carbon_intensity',\n",
    "    # pricing_df\n",
    "    'electricity_pricing',\n",
    "    'electricity_pricing_predicted_1',\n",
    "    'electricity_pricing_predicted_2',\n",
    "    'electricity_pricing_predicted_3'\n",
    "]\n",
    "bld_cols = [\n",
    "            'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "            'indoor_dry_bulb_temperature',\n",
    "            'average_unmet_cooling_setpoint_difference',\n",
    "            'indoor_relative_humidity', 'non_shiftable_load',\n",
    "            'dhw_demand', 'cooling_demand', 'heating_demand',\n",
    "            'solar_generation', 'occupant_count',\n",
    "            'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "            'indoor_dry_bulb_temperature_heating_set_point',\n",
    "            'hvac_mode'\n",
    "]\n",
    "wth_cols = [\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378295a5",
   "metadata": {},
   "source": [
    "### Deep Q-Learning Agent\n",
    "#### 1. Reward Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d178b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReward(RewardFunction):\n",
    "    def __init__(self, capacity: float):\n",
    "        self.capacity = capacity\n",
    "        self.prev_net_load = None\n",
    "\n",
    "    def __call__(self, obs: Mapping[str, float], action_frac: float) -> float:\n",
    "        # 1) Net load & cost\n",
    "        net_load   = obs['non_shiftable_load'] - action_frac*self.capacity # max: 11.25 kW\n",
    "        price = obs['electricity_pricing'] # 0.06605 or 0.03025\n",
    "        cost  = max(0.0, net_load) * price # deviation range: 0-11.25*0.06605 = ~0-0.75$\n",
    "        #norm_cost = cost / 0.75 # normalize to [0,1]\n",
    "\n",
    "        # return -(w_cost*cost + w_pen*comfort_penalty + w_emis*emis + w_ramp*ramp)        \n",
    "        return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec41d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoggerCallback(BaseCallback):\n",
    "    \"\"\"Logging State, Action, Reward per step and Loss per update phase.\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Will collect a dict per env-step\n",
    "        self.rows = []\n",
    "        # Loss values and their timesteps (global)\n",
    "        self.losses = []\n",
    "        self.loss_timesteps = []\n",
    "        # Completed episode returns (global list)\n",
    "        self.episode_rewards = []\n",
    "        # Placeholders for per-env tracking\n",
    "        self._current_ep_rewards = []         # sum of rewards in current episode per env\n",
    "        self._current_ep_counts = []          # episode index per env\n",
    "        self._current_step_in_episode = []    # step counter (0..T-1) per env\n",
    "\n",
    "        # DataFrames to populate at end\n",
    "        self.df = pd.DataFrame()\n",
    "        self.ep_df = pd.DataFrame()\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        try:\n",
    "            n_envs = self.training_env.num_envs\n",
    "        except AttributeError:\n",
    "            n_envs = 1\n",
    "        # initialize counters per sub-env\n",
    "        self._current_ep_rewards = [0.0] * n_envs\n",
    "        self._current_ep_counts = [1] * n_envs\n",
    "        self._current_step_in_episode = [0] * n_envs\n",
    "        super()._on_training_start()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        obs_vec = self.locals.get(\"new_obs\")\n",
    "        acts    = self.locals.get(\"actions\")\n",
    "        rews    = self.locals.get(\"rewards\")\n",
    "        dones   = self.locals.get(\"dones\")\n",
    "        step    = int(self.num_timesteps)\n",
    "\n",
    "        # log loss if present\n",
    "        loss_val = self.logger.name_to_value.get(\"train/loss\")\n",
    "        if loss_val is not None:\n",
    "            self.losses.append(float(loss_val))\n",
    "            self.loss_timesteps.append(step)\n",
    "\n",
    "        # iterate each sub-env\n",
    "        for idx, (obs, act, rew, done) in enumerate(zip(obs_vec, acts, rews, dones)):\n",
    "            # flatten observation\n",
    "            flat = obs.flatten().tolist()\n",
    "            # build row with metadata\n",
    "            row = {f\"x{i}\": flat[i] for i in range(len(flat))}\n",
    "            row.update({\n",
    "                \"env_id\": idx,\n",
    "                \"episode\": self._current_ep_counts[idx],\n",
    "                \"step_in_ep\": self._current_step_in_episode[idx],\n",
    "                \"action\": int(act),\n",
    "                \"reward\": float(rew),\n",
    "                \"global_step\": step\n",
    "            })\n",
    "            self.rows.append(row)\n",
    "\n",
    "            # accumulate per-episode reward\n",
    "            self._current_ep_rewards[idx] += float(rew)\n",
    "            # increment step in episode\n",
    "            self._current_step_in_episode[idx] += 1\n",
    "\n",
    "            # if end of episode for this env\n",
    "            if done:\n",
    "                # log reward and finalize episode\n",
    "                print(f\"Env {idx} Episode {self._current_ep_counts[idx]} done at global step {step}, total reward: {self._current_ep_rewards[idx]:.3f}\")\n",
    "                self.episode_rewards.append(self._current_ep_rewards[idx])\n",
    "                # reset for next episode\n",
    "                self._current_ep_rewards[idx] = 0.0\n",
    "                self._current_ep_counts[idx] += 1\n",
    "                self._current_step_in_episode[idx] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # build full-step DataFrame\n",
    "        self.df = pd.DataFrame(self.rows)\n",
    "        # build episodes summary DataFrame\n",
    "        self.ep_df = pd.DataFrame({\n",
    "            \"episode_global\": range(1, len(self.episode_rewards) + 1),\n",
    "            \"return\": self.episode_rewards\n",
    "        })\n",
    "        super()._on_training_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa10cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1\n",
      "Dataset '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1' copied to '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1/../../../../results/2025-08-18_17-44-18'\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.21e+03  |\n",
      "|    ep_rew_mean      | -1.73e+03 |\n",
      "|    exploration_rate | 0.72      |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 599       |\n",
      "|    total_timesteps  | 8828      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0003    |\n",
      "|    loss             | 3.22      |\n",
      "|    n_updates        | 27        |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     70\u001b[39m model = DQN(\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m     policy_kwargs=\u001b[38;5;28mdict\u001b[39m(net_arch=[\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m]),\n\u001b[32m     85\u001b[39m )\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# pick a small budget to smoke-test; increase later\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# ----------------- quick evaluation (one episode) -----------------\u001b[39;00m\n\u001b[32m     91\u001b[39m obs, _ = env.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/dqn/dqn.py:272\u001b[39m, in \u001b[36mDQN.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[32m    265\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    271\u001b[39m ) -> SelfDQN:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:335\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     rollout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout.continue_training:\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:567\u001b[39m, in \u001b[36mOffPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[39m\n\u001b[32m    564\u001b[39m actions, buffer_actions = \u001b[38;5;28mself\u001b[39m._sample_action(learning_starts, action_noise, env.num_envs)\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    570\u001b[39m num_collected_steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mCityLearnDQNWrapper.step\u001b[39m\u001b[34m(self, a)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# build CityLearn action format: list per building -> flat list for building 0\u001b[39;00m\n\u001b[32m     55\u001b[39m actions = [[frac] * \u001b[38;5;28mself\u001b[39m.N]  \u001b[38;5;66;03m# control building 0 only\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m obs_all, rewards, terminated, truncated, _info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m obs = np.array(obs_all[\u001b[32m0\u001b[39m], dtype=np.float32)\n\u001b[32m     59\u001b[39m reward = \u001b[38;5;28mfloat\u001b[39m(rewards[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/citylearn/citylearn.py:937\u001b[39m, in \u001b[36mCityLearnEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    934\u001b[39m actions = \u001b[38;5;28mself\u001b[39m._parse_actions(actions)\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m building, building_actions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.buildings, actions):\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[43mbuilding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuilding_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[38;5;28mself\u001b[39m.next_time_step()\n\u001b[32m    941\u001b[39m \u001b[38;5;66;03m#Currently at time_step t+1\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/citylearn/building.py:2617\u001b[39m, in \u001b[36mDynamicsBuilding.apply_actions\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   2615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m   2616\u001b[39m     \u001b[38;5;28msuper\u001b[39m().apply_actions(**kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2617\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_dynamics_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.simulate_dynamics:\n\u001b[32m   2620\u001b[39m         \u001b[38;5;28mself\u001b[39m.update_indoor_dry_bulb_temperature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/citylearn/building.py:2745\u001b[39m, in \u001b[36mLSTMDynamicsBuilding._update_dynamics_input\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2738\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Updates and returns the input time series for the dynmaics prediction model.\u001b[39;00m\n\u001b[32m   2739\u001b[39m \n\u001b[32m   2740\u001b[39m \u001b[33;03mUpdates the model input with the input variables for the current time step. \u001b[39;00m\n\u001b[32m   2741\u001b[39m \u001b[33;03mThe variables in the input will have length of lookback + 1.\u001b[39;00m\n\u001b[32m   2742\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2744\u001b[39m \u001b[38;5;66;03m# get relevant observations for the current time step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2745\u001b[39m observations = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiodic_normalization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2747\u001b[39m \u001b[38;5;66;03m# append current time step observations to model input\u001b[39;00m\n\u001b[32m   2748\u001b[39m \u001b[38;5;66;03m# leave out the oldest set of observations and keep only the previous n\u001b[39;00m\n\u001b[32m   2749\u001b[39m \u001b[38;5;66;03m# where n is the lookback + 1 (to include current time step observations)\u001b[39;00m\n\u001b[32m   2750\u001b[39m \u001b[38;5;28mself\u001b[39m.dynamics._model_input = [\n\u001b[32m   2751\u001b[39m     l[-\u001b[38;5;28mself\u001b[39m.dynamics.lookback:] + [(observations[k] - min_) / (max_ - min_)]\n\u001b[32m   2752\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m l, k, min_, max_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m   2757\u001b[39m     )\n\u001b[32m   2758\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/citylearn/building.py:896\u001b[39m, in \u001b[36mBuilding.observations\u001b[39m\u001b[34m(self, include_all, normalize, periodic_normalization, check_limits)\u001b[39m\n\u001b[32m    893\u001b[39m check_limits = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m check_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m check_limits\n\u001b[32m    895\u001b[39m observations = {}\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_observations_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_all:\n\u001b[32m    899\u001b[39m     valid_observations = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(data.keys()) | \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.active_observations))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/citylearn/building.py:1184\u001b[39m, in \u001b[36mBuilding._get_observations_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1152\u001b[39m             load_profile =  wm.washing_machine_simulation.load_profile[\u001b[38;5;28mself\u001b[39m.time_step]\n\u001b[32m   1154\u001b[39m     washing_machines_dict[washing_machine_name] = {\n\u001b[32m   1155\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwm_start_time_step\u001b[39m\u001b[33m\"\u001b[39m: start_time_step,\n\u001b[32m   1156\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwm_end_time_step\u001b[39m\u001b[33m\"\u001b[39m: end_time_step,\n\u001b[32m   1157\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mload_profile\u001b[39m\u001b[33m\"\u001b[39m: load_profile,\n\u001b[32m   1158\u001b[39m     }\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m   1161\u001b[39m     **{\n\u001b[32m   1162\u001b[39m         k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m): \u001b[38;5;28mself\u001b[39m.energy_simulation.\u001b[34m__getattr__\u001b[39m(k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m.time_step] \n\u001b[32m   1163\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m.energy_simulation).items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np.ndarray)\n\u001b[32m   1164\u001b[39m     },\n\u001b[32m   1165\u001b[39m     **{\n\u001b[32m   1166\u001b[39m         k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m): \u001b[38;5;28mself\u001b[39m.weather.\u001b[34m__getattr__\u001b[39m(k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m.time_step] \n\u001b[32m   1167\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m.weather).items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np.ndarray)\n\u001b[32m   1168\u001b[39m     },\n\u001b[32m   1169\u001b[39m     **{\n\u001b[32m   1170\u001b[39m         k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m): \u001b[38;5;28mself\u001b[39m.pricing.\u001b[34m__getattr__\u001b[39m(k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m.time_step] \n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m.pricing).items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np.ndarray)\n\u001b[32m   1172\u001b[39m     },\n\u001b[32m   1173\u001b[39m     **{\n\u001b[32m   1174\u001b[39m         k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m): \u001b[38;5;28mself\u001b[39m.carbon_intensity.\u001b[34m__getattr__\u001b[39m(k.lstrip(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m.time_step] \n\u001b[32m   1175\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m.carbon_intensity).items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np.ndarray)\n\u001b[32m   1176\u001b[39m     },\n\u001b[32m   1177\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msolar_generation\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mself\u001b[39m.solar_generation[\u001b[38;5;28mself\u001b[39m.time_step]),\n\u001b[32m   1178\u001b[39m     **{\n\u001b[32m   1179\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcooling_storage_soc\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28mself\u001b[39m.cooling_storage.soc[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1180\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mheating_storage_soc\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28mself\u001b[39m.heating_storage.soc[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1181\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdhw_storage_soc\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28mself\u001b[39m.dhw_storage.soc[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1182\u001b[39m         \u001b[33m'\u001b[39m\u001b[33melectrical_storage_soc\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28mself\u001b[39m.electrical_storage.soc[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1183\u001b[39m     },\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcooling_demand\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.__energy_from_cooling_device[\u001b[38;5;28mself\u001b[39m.time_step] + \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.cooling_storage.energy_balance[\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime_step\u001b[49m], \u001b[32m0.0\u001b[39m)),\n\u001b[32m   1185\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mheating_demand\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.__energy_from_heating_device[\u001b[38;5;28mself\u001b[39m.time_step] + \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.heating_storage.energy_balance[\u001b[38;5;28mself\u001b[39m.time_step], \u001b[32m0.0\u001b[39m)),\n\u001b[32m   1186\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdhw_demand\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.__energy_from_dhw_device[\u001b[38;5;28mself\u001b[39m.time_step] + \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.dhw_storage.energy_balance[\u001b[38;5;28mself\u001b[39m.time_step], \u001b[32m0.0\u001b[39m)),\n\u001b[32m   1187\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnet_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.net_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step - \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.time_step > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m],\n\u001b[32m   1188\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcooling_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.cooling_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1189\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mheating_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.heating_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1190\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdhw_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.dhw_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1191\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcooling_storage_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.cooling_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1192\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mheating_storage_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.heating_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1193\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdhw_storage_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.dhw_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1194\u001b[39m     \u001b[33m'\u001b[39m\u001b[33melectrical_storage_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.electrical_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1195\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwashing_machine_electricity_consumption\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.washing_machines_electricity_consumption[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1196\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcooling_device_efficiency\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.cooling_device.get_cop(\u001b[38;5;28mself\u001b[39m.weather.outdoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m.time_step], heating=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1197\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mheating_device_efficiency\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.heating_device.get_cop(\u001b[38;5;28mself\u001b[39m.weather.outdoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m.time_step], heating=\u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[32m   1198\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.heating_device, HeatPump) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heating_device.efficiency,\n\u001b[32m   1199\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdhw_device_efficiency\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.dhw_device.get_cop(\u001b[38;5;28mself\u001b[39m.weather.outdoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m.time_step], heating=\u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.dhw_device, HeatPump) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dhw_device.efficiency,\n\u001b[32m   1201\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mindoor_dry_bulb_temperature_cooling_set_point\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.energy_simulation.indoor_dry_bulb_temperature_cooling_set_point[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1202\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mindoor_dry_bulb_temperature_heating_set_point\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.energy_simulation.indoor_dry_bulb_temperature_heating_set_point[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1203\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mindoor_dry_bulb_temperature_cooling_delta\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.energy_simulation.indoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m.time_step] - \u001b[38;5;28mself\u001b[39m.energy_simulation.indoor_dry_bulb_temperature_cooling_set_point[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1204\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mindoor_dry_bulb_temperature_heating_delta\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.energy_simulation.indoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m.time_step] - \u001b[38;5;28mself\u001b[39m.energy_simulation.indoor_dry_bulb_temperature_heating_set_point[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1205\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcomfort_band\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.energy_simulation.comfort_band[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1206\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moccupant_count\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.energy_simulation.occupant_count[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1207\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpower_outage\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.__power_outage_signal[\u001b[38;5;28mself\u001b[39m.time_step],\n\u001b[32m   1208\u001b[39m     \u001b[33m'\u001b[39m\u001b[33melectric_vehicles_chargers_dict\u001b[39m\u001b[33m'\u001b[39m: electric_vehicle_chargers_dict,\n\u001b[32m   1209\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwashing_machines_dict\u001b[39m\u001b[33m'\u001b[39m: washing_machines_dict,\n\u001b[32m   1210\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/citylearn/base.py:190\u001b[39m, in \u001b[36mEnvironment.time_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\":py:class:`citylearn.base.EpisodeTracker` object used to keep track of \u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    current episode time steps for reading observations from data files.\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__time_step_ratio\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtime_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    192\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Current environment time step.\"\"\"\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__time_step\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Minimal DQN on standard CityLearnEnv (single building, single action)\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "\n",
    "# --- wrapper: single-agent SB3 env on top of CityLearnEnv + action discretization ---\n",
    "class CityLearnDQNWrapper(gym.Env):\n",
    "    \"\"\"Wrap standard CityLearnEnv for SB3 DQN:\n",
    "       - single central agent, we control building 0\n",
    "       - discretize the [-1, 1] storage action into n_bins\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, cl_env: CityLearnEnv, n_bins: int = 5):\n",
    "        super().__init__()\n",
    "        self.cl = cl_env\n",
    "        self.n_bins = int(n_bins)\n",
    "        assert self.n_bins >= 2, \"n_bins must be >= 2\"\n",
    "\n",
    "        # Build discrete action space\n",
    "        self.action_space = spaces.Discrete(self.n_bins)\n",
    "\n",
    "        # Infer observation shape (use building 0 obs)\n",
    "        self.cl.reset()\n",
    "        obs0 = np.array(self.cl.observations[0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=obs0.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Precompute action broadcast size (how many controls for bldg 0)\n",
    "        self.N = self.cl.action_space[0].shape[0]  # should be 1 if only electrical_storage is active\n",
    "\n",
    "    def _map_discrete_to_frac(self, a: int) -> float:\n",
    "        # map {0..n_bins-1} -> [-1, 1]\n",
    "        if self.n_bins == 1:\n",
    "            return 0.0\n",
    "        return -1.0 + 2.0 * (a / (self.n_bins - 1))\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.cl.reset(seed=seed)\n",
    "        obs = np.array(self.cl.observations[0], dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, a: int):\n",
    "        # map discrete to continuous fraction\n",
    "        frac = float(np.clip(self._map_discrete_to_frac(int(a)), -1.0, 1.0))\n",
    "        # build CityLearn action format: list per building -> flat list for building 0\n",
    "        actions = [[frac] * self.N]  # control building 0 only\n",
    "        obs_all, rewards, terminated, truncated, _info = self.cl.step(actions)\n",
    "\n",
    "        obs = np.array(obs_all[0], dtype=np.float32)\n",
    "        reward = float(rewards[0])\n",
    "        term = bool(terminated)\n",
    "        trunc = bool(truncated)\n",
    "        return obs, reward, term, trunc, {}\n",
    "\n",
    "# ----------------- build env -----------------\n",
    "\n",
    "cl_env = CityLearnEnv(schema, central_agent=True)\n",
    "env = Monitor(CityLearnDQNWrapper(cl_env, n_bins=5))\n",
    "\n",
    "# Train env\n",
    "train_env = SubprocVecEnv(env)\n",
    "train_env = VecMonitor(train_env)\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# New eval env\n",
    "eval_env = DummyVecEnv([lambda: Monitor(CityLearnEnv(building_df=bld, pricing_df=prc, weather_df=wth, carbon_df=car, n_bins=5))])\n",
    "eval_env = VecMonitor(eval_env) \n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "\n",
    "# Evaluate every 2k steps over 5 episodes\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    log_path=\"logs/eval/\",\n",
    "    best_model_save_path=\"logs/best_model/\",\n",
    "    #eval_freq=len(building_data), # run evaluation every 5k timesteps, TODO: change to 5_000\n",
    "    eval_freq=1000, # run evaluation every n timesteps, TODO: change n to 5_000\n",
    "    n_eval_episodes=5,       # average over 5 full episodes\n",
    "    deterministic=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_callback = TrainLoggerCallback()\n",
    "\n",
    "# ----------------- train DQN (super simple) -----------------\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=256,\n",
    "    learning_starts=2_000,\n",
    "    train_freq=256,\n",
    "    target_update_interval=1_000,\n",
    "    gamma=0.98,\n",
    "    exploration_fraction=0.3,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    ")\n",
    "\n",
    "# Model training\n",
    "#model.learn(total_timesteps=100_000)\n",
    "T = len(building_data) # 1 episode = 2208 timesteps\n",
    "num_episodes = 10 # 10 episodes\n",
    "model.learn(\n",
    "    total_timesteps=num_episodes * T,\n",
    "    callback=CallbackList([train_callback, eval_callback])\n",
    ")\n",
    "\n",
    "print(\"Eval timesteps:\", eval_callback.evaluations_timesteps)\n",
    "print(\"Eval results   :\", eval_callback.evaluations_results)\n",
    "\n",
    "\n",
    "print(\"---------------Train callback: \\n\", train_callback.df)\n",
    "print(\"---------------Episode rewards: \\n\", train_callback.ep_df)\n",
    "\n",
    "\n",
    "# Evaluation results\n",
    "all_rewards = eval_callback.evaluations_results # List of lists: each element are rewards from an eval round\n",
    "eval_steps  = eval_callback.evaluations_timesteps # timesteps at which the evaluations were run\n",
    "\n",
    "# Mean reward per round\n",
    "mean_rewards = [np.mean(r) for r in all_rewards]\n",
    "\n",
    "# ----------------- quick evaluation (one episode) -----------------\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "ret = 0.0\n",
    "while not done:\n",
    "    action = model.predict(obs, deterministic=True)[0]\n",
    "    obs, r, term, trunc, _ = env.step(action)\n",
    "    ret += r\n",
    "    done = term or trunc\n",
    "print(f\"Episode return: {ret:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (citylearn_env)",
   "language": "python",
   "name": "citylearn_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
