{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beca837",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://haicore-jupyter.scc.kit.edu/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'https://haicore-jupyter.scc.kit.edu/'. Verify the server is running and reachable. (JupyterHub server no longer running at /user/cj9272/api/kernels. Restart the server at https://haicore-jupyter.scc.kit.edu/jhub/hub/spawn/cj9272).)."
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b112d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# The environment we will be working with\n",
    "%pip install CityLearn==2.1.2\n",
    "\n",
    "# For participant interactions (buttons)\n",
    "%pip install ipywidgets\n",
    "\n",
    "# To generate static figures\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "\n",
    "# Provide standard RL algorithms\n",
    "%pip install stable-baselines3\n",
    "\n",
    "# Enable gym compatibility with later stable-baselines3 versions\n",
    "%pip install shimmy\n",
    "\n",
    "# Results submission\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Pip:   \", subprocess.run([\"which\",\"pip\"], capture_output=True, text=True).stdout)\n",
    "\n",
    "# System operations\n",
    "import os\n",
    "\n",
    "# Type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "\n",
    "# CityLearn\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction, SolarPenaltyReward\n",
    "\n",
    "# Baseline RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "# set all plotted figures without margins\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b053b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "DATASET_NAME = 'citylearn_challenge_2023_phase_3_1'\n",
    "schema = DataSet().get_schema(DATASET_NAME)\n",
    "print(schema['root_directory'])\n",
    "\n",
    "# Building\n",
    "#root_directory = schema['root_directory']\n",
    "root_directory = 'Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "building_name = 'Building_1'\n",
    "# Weather data\n",
    "filename = schema['buildings'][building_name]['weather']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "weather_data = pd.read_csv(filepath)\n",
    "# Pricing data (simple)\n",
    "filename = schema['buildings'][building_name]['pricing']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "pricing_data = pd.read_csv(filepath)\n",
    "# Carbon Intensity data\n",
    "filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "carbon_intensity_data = pd.read_csv(filepath)\n",
    "# building data\n",
    "filename = schema['buildings'][building_name]['energy_simulation']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "building_data = pd.read_csv(filepath)\n",
    "\n",
    "# Display building data\n",
    "# display(building_data.head())\n",
    "# display(building_data.describe(include='all'))\n",
    "\n",
    "bld = building_data.copy()\n",
    "wth = weather_data.copy()\n",
    "prc = pricing_data.copy()\n",
    "car = carbon_intensity_data.copy()\n",
    "\n",
    "print(building_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [\n",
    "    # building_df\n",
    "    'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "    'indoor_dry_bulb_temperature',\n",
    "    'average_unmet_cooling_setpoint_difference',\n",
    "    'indoor_relative_humidity',\n",
    "    'non_shiftable_load', 'dhw_demand',\n",
    "    'cooling_demand', 'heating_demand',\n",
    "    'solar_generation', 'occupant_count',\n",
    "    'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "    'indoor_dry_bulb_temperature_heating_set_point', 'hvac_mode',\n",
    "    # weather_df\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance',\n",
    "    'outdoor_dry_bulb_temperature_predicted_1',\n",
    "    'outdoor_dry_bulb_temperature_predicted_2',\n",
    "    'outdoor_dry_bulb_temperature_predicted_3',\n",
    "    'outdoor_relative_humidity_predicted_1',\n",
    "    'outdoor_relative_humidity_predicted_2',\n",
    "    'outdoor_relative_humidity_predicted_3',\n",
    "    'diffuse_solar_irradiance_predicted_1',\n",
    "    'diffuse_solar_irradiance_predicted_2',\n",
    "    'diffuse_solar_irradiance_predicted_3',\n",
    "    'direct_solar_irradiance_predicted_1',\n",
    "    'direct_solar_irradiance_predicted_2',\n",
    "    'direct_solar_irradiance_predicted_3',\n",
    "    # carbon_df \n",
    "    'carbon_intensity',\n",
    "    # pricing_df\n",
    "    'electricity_pricing',\n",
    "    'electricity_pricing_predicted_1',\n",
    "    'electricity_pricing_predicted_2',\n",
    "    'electricity_pricing_predicted_3'\n",
    "]\n",
    "bld_cols = [\n",
    "            'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "            'indoor_dry_bulb_temperature',\n",
    "            'average_unmet_cooling_setpoint_difference',\n",
    "            'indoor_relative_humidity', 'non_shiftable_load',\n",
    "            'dhw_demand', 'cooling_demand', 'heating_demand',\n",
    "            'solar_generation', 'occupant_count',\n",
    "            'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "            'indoor_dry_bulb_temperature_heating_set_point',\n",
    "            'hvac_mode'\n",
    "]\n",
    "wth_cols = [\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378295a5",
   "metadata": {},
   "source": [
    "### Deep Q-Learning Agent\n",
    "#### 1. Reward Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d178b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReward(RewardFunction):\n",
    "    def __init__(self, capacity: float):\n",
    "        self.capacity = capacity\n",
    "        self.prev_net_load = None\n",
    "\n",
    "    def __call__(self, obs: Mapping[str, float], action_frac: float) -> float:\n",
    "        # 1) Net load & cost\n",
    "        net_load   = obs['non_shiftable_load'] - action_frac*self.capacity # max: 11.25 kW\n",
    "        price = obs['electricity_pricing'] # 0.06605 or 0.03025\n",
    "        cost  = max(0.0, net_load) * price # deviation range: 0-11.25*0.06605 = ~0-0.75$\n",
    "        #norm_cost = cost / 0.75 # normalize to [0,1]\n",
    "\n",
    "        # return -(w_cost*cost + w_pen*comfort_penalty + w_emis*emis + w_ramp*ramp)        \n",
    "        return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoggerCallback(BaseCallback):\n",
    "    \"\"\"Logging State, Action, Reward per step and Loss per update phase.\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Will collect a dict per env-step\n",
    "        self.rows = []\n",
    "        # Loss values and their timesteps (global)\n",
    "        self.losses = []\n",
    "        self.loss_timesteps = []\n",
    "        # Completed episode returns (global list)\n",
    "        self.episode_rewards = []\n",
    "        # Placeholders for per-env tracking\n",
    "        self._current_ep_rewards = []         # sum of rewards in current episode per env\n",
    "        self._current_ep_counts = []          # episode index per env\n",
    "        self._current_step_in_episode = []    # step counter (0..T-1) per env\n",
    "\n",
    "        # DataFrames to populate at end\n",
    "        self.df = pd.DataFrame()\n",
    "        self.ep_df = pd.DataFrame()\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        try:\n",
    "            n_envs = self.training_env.num_envs\n",
    "        except AttributeError:\n",
    "            n_envs = 1\n",
    "        # initialize counters per sub-env\n",
    "        self._current_ep_rewards = [0.0] * n_envs\n",
    "        self._current_ep_counts = [1] * n_envs\n",
    "        self._current_step_in_episode = [0] * n_envs\n",
    "        super()._on_training_start()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        obs_vec = self.locals.get(\"new_obs\")\n",
    "        acts    = self.locals.get(\"actions\")\n",
    "        rews    = self.locals.get(\"rewards\")\n",
    "        dones   = self.locals.get(\"dones\")\n",
    "        step    = int(self.num_timesteps)\n",
    "\n",
    "        # log loss if present\n",
    "        loss_val = self.logger.name_to_value.get(\"train/loss\")\n",
    "        if loss_val is not None:\n",
    "            self.losses.append(float(loss_val))\n",
    "            self.loss_timesteps.append(step)\n",
    "\n",
    "        # iterate each sub-env\n",
    "        for idx, (obs, act, rew, done) in enumerate(zip(obs_vec, acts, rews, dones)):\n",
    "            # flatten observation\n",
    "            flat = obs.flatten().tolist()\n",
    "            # build row with metadata\n",
    "            row = {f\"x{i}\": flat[i] for i in range(len(flat))}\n",
    "            row.update({\n",
    "                \"env_id\": idx,\n",
    "                \"episode\": self._current_ep_counts[idx],\n",
    "                \"step_in_ep\": self._current_step_in_episode[idx],\n",
    "                \"action\": int(act),\n",
    "                \"reward\": float(rew),\n",
    "                \"global_step\": step\n",
    "            })\n",
    "            self.rows.append(row)\n",
    "\n",
    "            # accumulate per-episode reward\n",
    "            self._current_ep_rewards[idx] += float(rew)\n",
    "            # increment step in episode\n",
    "            self._current_step_in_episode[idx] += 1\n",
    "\n",
    "            # if end of episode for this env\n",
    "            if done:\n",
    "                # log reward and finalize episode\n",
    "                print(f\"Env {idx} Episode {self._current_ep_counts[idx]} done at global step {step}, total reward: {self._current_ep_rewards[idx]:.3f}\")\n",
    "                self.episode_rewards.append(self._current_ep_rewards[idx])\n",
    "                # reset for next episode\n",
    "                self._current_ep_rewards[idx] = 0.0\n",
    "                self._current_ep_counts[idx] += 1\n",
    "                self._current_step_in_episode[idx] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # build full-step DataFrame\n",
    "        self.df = pd.DataFrame(self.rows)\n",
    "        # build episodes summary DataFrame\n",
    "        self.ep_df = pd.DataFrame({\n",
    "            \"episode_global\": range(1, len(self.episode_rewards) + 1),\n",
    "            \"return\": self.episode_rewards\n",
    "        })\n",
    "        super()._on_training_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa10cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal DQN on standard CityLearnEnv (single building, single action)\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "# --- keep only electrical storage so we have 1D continuous action to discretize ---\n",
    "def keep_only_electrical_storage(schema: dict) -> dict:\n",
    "    if 'actions' in schema:\n",
    "        for a in list(schema['actions'].keys()):\n",
    "            schema['actions'][a]['active'] = (a == 'electrical_storage')\n",
    "    if 'buildings' in schema:\n",
    "        for b in schema['buildings']:\n",
    "            if 'actions' in b:\n",
    "                for a in list(b['actions'].keys()):\n",
    "                    b['actions'][a]['active'] = (a == 'electrical_storage')\n",
    "    return schema\n",
    "\n",
    "# --- wrapper: single-agent SB3 env on top of CityLearnEnv + action discretization ---\n",
    "class CityLearnDQNWrapper(gym.Env):\n",
    "    \"\"\"Wrap standard CityLearnEnv for SB3 DQN:\n",
    "       - single central agent, we control building 0\n",
    "       - discretize the [-1, 1] storage action into n_bins\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, cl_env: CityLearnEnv, n_bins: int = 5):\n",
    "        super().__init__()\n",
    "        self.cl = cl_env\n",
    "        self.n_bins = int(n_bins)\n",
    "        assert self.n_bins >= 2, \"n_bins must be >= 2\"\n",
    "\n",
    "        # Build discrete action space\n",
    "        self.action_space = spaces.Discrete(self.n_bins)\n",
    "\n",
    "        # Infer observation shape (use building 0 obs)\n",
    "        self.cl.reset()\n",
    "        obs0 = np.array(self.cl.observations[0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=obs0.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Precompute action broadcast size (how many controls for bldg 0)\n",
    "        self.N = self.cl.action_space[0].shape[0]  # should be 1 if only electrical_storage is active\n",
    "\n",
    "    def _map_discrete_to_frac(self, a: int) -> float:\n",
    "        # map {0..n_bins-1} -> [-1, 1]\n",
    "        if self.n_bins == 1:\n",
    "            return 0.0\n",
    "        return -1.0 + 2.0 * (a / (self.n_bins - 1))\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.cl.reset(seed=seed)\n",
    "        obs = np.array(self.cl.observations[0], dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, a: int):\n",
    "        # map discrete to continuous fraction\n",
    "        frac = float(np.clip(self._map_discrete_to_frac(int(a)), -1.0, 1.0))\n",
    "        # build CityLearn action format: list per building -> flat list for building 0\n",
    "        actions = [[frac] * self.N]  # control building 0 only\n",
    "        obs_all, rewards, terminated, truncated, _info = self.cl.step(actions)\n",
    "\n",
    "        obs = np.array(obs_all[0], dtype=np.float32)\n",
    "        reward = float(rewards[0])\n",
    "        term = bool(terminated)\n",
    "        trunc = bool(truncated)\n",
    "        return obs, reward, term, trunc, {}\n",
    "\n",
    "# ----------------- build env -----------------\n",
    "dataset = DataSet()\n",
    "schema = dataset.get_schema('citylearn_challenge_2023_phase_3_1')\n",
    "schema = keep_only_electrical_storage(schema)\n",
    "cl_env = CityLearnEnv(schema, central_agent=True)\n",
    "\n",
    "env = Monitor(CityLearnDQNWrapper(cl_env, n_bins=5))\n",
    "\n",
    "# ----------------- train DQN (super simple) -----------------\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=256,\n",
    "    learning_starts=2_000,\n",
    "    train_freq=256,\n",
    "    target_update_interval=1_000,\n",
    "    gamma=0.98,\n",
    "    exploration_fraction=0.3,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256], activation_fn=None),\n",
    ")\n",
    "\n",
    "# pick a small budget to smoke-test; increase later\n",
    "model.learn(total_timesteps=100_000)\n",
    "\n",
    "# ----------------- quick evaluation (one episode) -----------------\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "ret = 0.0\n",
    "while not done:\n",
    "    action = model.predict(obs, deterministic=True)[0]\n",
    "    obs, r, term, trunc, _ = env.step(action)\n",
    "    ret += r\n",
    "    done = term or trunc\n",
    "print(f\"Episode return: {ret:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ab62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training reward\n",
    "window = 100\n",
    "smooth = train_callback.ep_df[\"return\"].rolling(window, min_periods=1).mean()\n",
    "#train_df = train_callback.df  # your TrainLoggerCallback should have produced this\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(train_callback.ep_df[\"episode_global\"], smooth, label=f\"{window}-Episode MA\", color=\"C0\")\n",
    "#plt.plot(train_df[\"step\"], train_df[\"reward\"], color=\"C0\")\n",
    "plt.title(\"Moving Average of the Reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward (moving average)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "n_episodes = len(train_callback.ep_df)\n",
    "print(f\"Total episodes: {n_episodes}\")\n",
    "\"\"\"\n",
    "# Training loss (if available)\n",
    "if hasattr(train_callback, \"losses\") and len(train_callback.losses) > 0:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(train_callback.loss_timesteps,\n",
    "             train_callback.losses,\n",
    "             marker='.', linestyle='-',\n",
    "             alpha=0.7, color=\"C1\")\n",
    "    plt.title(\"Train Loss over time\")\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "train_df = train_callback.df  \n",
    "\n",
    "# Actions (raw + moving average)\n",
    "plt.figure(figsize=(6, 3))\n",
    "# raw scatter\n",
    "plt.scatter(\n",
    "    train_df[\"global_step\"],\n",
    "    train_df[\"action\"],\n",
    "    c=\"lightgray\", s=5, alpha=0.4,\n",
    "    label=\"raw actions\"\n",
    ")\n",
    "# moving average\n",
    "window = 500\n",
    "train_df[\"action_ma\"] = train_df[\"action\"].rolling(window, min_periods=1).mean()\n",
    "plt.plot(\n",
    "    train_df[\"global_step\"],\n",
    "    train_df[\"action_ma\"],\n",
    "    color=\"C2\", lw=2,\n",
    "    label=f\"{window}-step MA\"\n",
    ")\n",
    "plt.title(\"Actions over time (raw + moving average)\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Discrete Action\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation results\n",
    "all_eval = s.evaluations_results        # list of lists\n",
    "steps    = eval_callback.evaluations_timesteps      # list of ints\n",
    "\n",
    "# Compute per‐evaluation mean/std\n",
    "mean_eval = [float(np.mean(r)) for r in all_eval]\n",
    "std_eval  = [float(np.std(r))  for r in all_eval]\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.errorbar(\n",
    "    steps, mean_eval,\n",
    "    yerr=std_eval,\n",
    "    fmt='-o', capsize=3, color=\"C3\",\n",
    "    label=\"Eval mean ±1σ\"\n",
    ")\n",
    "plt.title(\"Mean Evaluation Reward over time\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (citylearn_env)",
   "language": "python",
   "name": "citylearn_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
