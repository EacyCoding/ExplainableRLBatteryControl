{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0beca837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b112d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%capture\\n\\n# The environment we will be working with\\n%pip install CityLearn==2.1.2\\n\\n# For participant interactions (buttons)\\n%pip install ipywidgets\\n\\n# To generate static figures\\n%pip install matplotlib\\n%pip install seaborn\\n\\n# Provide standard RL algorithms\\n%pip install stable-baselines3\\n\\n# Enable gym compatibility with later stable-baselines3 versions\\n%pip install shimmy\\n\\n# Results submission\\n%pip install requests\\n%pip install beautifulsoup4\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%capture\n",
    "\n",
    "# The environment we will be working with\n",
    "%pip install CityLearn==2.1.2\n",
    "\n",
    "# For participant interactions (buttons)\n",
    "%pip install ipywidgets\n",
    "\n",
    "# To generate static figures\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "\n",
    "# Provide standard RL algorithms\n",
    "%pip install stable-baselines3\n",
    "\n",
    "# Enable gym compatibility with later stable-baselines3 versions\n",
    "%pip install shimmy\n",
    "\n",
    "# Results submission\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81f853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /hkfs/home/haicore/iai/cj9272/citylearn_env/bin/python\n",
      "Pip:    /software/all/jupyter/ai/2025-05-23/bin/pip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Pip:   \", subprocess.run([\"which\",\"pip\"], capture_output=True, text=True).stdout)\n",
    "\n",
    "# System operations\n",
    "import os\n",
    "\n",
    "# Type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "\n",
    "# CityLearn\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction, SolarPenaltyReward\n",
    "\n",
    "# Baseline RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "# set all plotted figures without margins\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b053b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Go here /home/iai/cj9272/.cache/citylearn/v2.4.1/datasets/citylearn_challenge_2023_phase_3_1/schema.json \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'hour', 'day_type', 'daylight_savings_status', 'indoor_dry_bulb_temperature', 'average_unmet_cooling_setpoint_difference', 'indoor_relative_humidity', 'non_shiftable_load', 'dhw_demand', 'cooling_demand', 'heating_demand', 'solar_generation', 'occupant_count', 'indoor_dry_bulb_temperature_cooling_set_point', 'indoor_dry_bulb_temperature_heating_set_point', 'hvac_mode']\n"
     ]
    }
   ],
   "source": [
    "# --- keep only electrical storage so we have 1D continuous action to discretize ---\n",
    "def keep_only_electrical_storage(schema: dict) -> dict:\n",
    "    if 'actions' in schema:\n",
    "        for a in list(schema['actions'].keys()):\n",
    "            schema['actions'][a]['active'] = (a == 'electrical_storage')\n",
    "    if 'buildings' in schema:\n",
    "        for b in schema['buildings']:\n",
    "            if 'actions' in b:\n",
    "                for a in list(b['actions'].keys()):\n",
    "                    b['actions'][a]['active'] = (a == 'electrical_storage')\n",
    "    return schema\n",
    "# Dataset\n",
    "DATASET_NAME = 'citylearn_challenge_2023_phase_3_1'\n",
    "schema = DataSet().get_schema(DATASET_NAME)\n",
    "schema['root_directory'] = r'/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "schema = keep_only_electrical_storage(schema) # Activate only the electrical storage control (fix \"Expected 18 actions but got 1\")\n",
    "# Set reward function\n",
    "schema['reward_function'] = { # CostReward Function\n",
    "    'type': 'citylearn.reward_function.CostReward',\n",
    "    'attributes': {}\n",
    "}\n",
    "# Set pricing file\n",
    "price_file = 'pricing_germany_2023_june_to_august.csv'  # Pricing CSV\n",
    "if 'buildings' not in schema:\n",
    "    raise RuntimeError(\"schema does not contain 'buildings' (make sure schema is loaded first)\")\n",
    "for bname, bconf in schema['buildings'].items():\n",
    "    bconf['pricing'] = price_file\n",
    "\n",
    "# Building\n",
    "#root_directory = schema['root_directory']\n",
    "root_directory = 'Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "building_name = 'Building_1'\n",
    "# Weather data\n",
    "filename = schema['buildings'][building_name]['weather']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "weather_data = pd.read_csv(filepath)\n",
    "# Pricing data (simple)\n",
    "filename = schema['buildings'][building_name]['pricing']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "pricing_data = pd.read_csv(filepath)\n",
    "# Carbon Intensity data\n",
    "filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "carbon_intensity_data = pd.read_csv(filepath)\n",
    "# building data\n",
    "filename = schema['buildings'][building_name]['energy_simulation']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "building_data = pd.read_csv(filepath)\n",
    "\n",
    "# Display building data\n",
    "# display(building_data.head())\n",
    "# display(building_data.describe(include='all'))\n",
    "\n",
    "bld = building_data.copy()\n",
    "wth = weather_data.copy()\n",
    "prc = pricing_data.copy()\n",
    "car = carbon_intensity_data.copy()\n",
    "\n",
    "print(building_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcff422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [\n",
    "    # building_df\n",
    "    'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "    'indoor_dry_bulb_temperature',\n",
    "    'average_unmet_cooling_setpoint_difference',\n",
    "    'indoor_relative_humidity',\n",
    "    'non_shiftable_load', 'dhw_demand',\n",
    "    'cooling_demand', 'heating_demand',\n",
    "    'solar_generation', 'occupant_count',\n",
    "    'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "    'indoor_dry_bulb_temperature_heating_set_point', 'hvac_mode',\n",
    "    # weather_df\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance',\n",
    "    'outdoor_dry_bulb_temperature_predicted_1',\n",
    "    'outdoor_dry_bulb_temperature_predicted_2',\n",
    "    'outdoor_dry_bulb_temperature_predicted_3',\n",
    "    'outdoor_relative_humidity_predicted_1',\n",
    "    'outdoor_relative_humidity_predicted_2',\n",
    "    'outdoor_relative_humidity_predicted_3',\n",
    "    'diffuse_solar_irradiance_predicted_1',\n",
    "    'diffuse_solar_irradiance_predicted_2',\n",
    "    'diffuse_solar_irradiance_predicted_3',\n",
    "    'direct_solar_irradiance_predicted_1',\n",
    "    'direct_solar_irradiance_predicted_2',\n",
    "    'direct_solar_irradiance_predicted_3',\n",
    "    # carbon_df \n",
    "    'carbon_intensity',\n",
    "    # pricing_df\n",
    "    'electricity_pricing',\n",
    "    'electricity_pricing_predicted_1',\n",
    "    'electricity_pricing_predicted_2',\n",
    "    'electricity_pricing_predicted_3'\n",
    "]\n",
    "bld_cols = [\n",
    "            'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "            'indoor_dry_bulb_temperature',\n",
    "            'average_unmet_cooling_setpoint_difference',\n",
    "            'indoor_relative_humidity', 'non_shiftable_load',\n",
    "            'dhw_demand', 'cooling_demand', 'heating_demand',\n",
    "            'solar_generation', 'occupant_count',\n",
    "            'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "            'indoor_dry_bulb_temperature_heating_set_point',\n",
    "            'hvac_mode'\n",
    "]\n",
    "wth_cols = [\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378295a5",
   "metadata": {},
   "source": [
    "### Deep Q-Learning Agent\n",
    "#### 1. Reward Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d178b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReward(RewardFunction):\n",
    "    def __init__(self, capacity: float):\n",
    "        self.capacity = capacity\n",
    "        self.prev_net_load = None\n",
    "\n",
    "    def __call__(self, obs: Mapping[str, float], action_frac: float) -> float:\n",
    "        # 1) Net load & cost\n",
    "        net_load   = obs['non_shiftable_load'] - action_frac*self.capacity # max: 11.25 kW\n",
    "        price = obs['electricity_pricing'] # 0.06605 or 0.03025\n",
    "        cost  = max(0.0, net_load) * price # deviation range: 0-11.25*0.06605 = ~0-0.75$\n",
    "        #norm_cost = cost / 0.75 # normalize to [0,1]\n",
    "\n",
    "        # return -(w_cost*cost + w_pen*comfort_penalty + w_emis*emis + w_ramp*ramp)        \n",
    "        return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec41d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoggerCallback(BaseCallback):\n",
    "    \"\"\"Logging State, Action, Reward per step and Loss per update phase.\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Will collect a dict per env-step\n",
    "        self.rows = []\n",
    "        # Loss values and their timesteps (global)\n",
    "        self.losses = []\n",
    "        self.loss_timesteps = []\n",
    "        # Completed episode returns (global list)\n",
    "        self.episode_rewards = []\n",
    "        # Placeholders for per-env tracking\n",
    "        self._current_ep_rewards = []         # sum of rewards in current episode per env\n",
    "        self._current_ep_counts = []          # episode index per env\n",
    "        self._current_step_in_episode = []    # step counter (0..T-1) per env\n",
    "\n",
    "        # DataFrames to populate at end\n",
    "        self.df = pd.DataFrame()\n",
    "        self.ep_df = pd.DataFrame()\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        try:\n",
    "            n_envs = self.training_env.num_envs\n",
    "        except AttributeError:\n",
    "            n_envs = 1\n",
    "        # initialize counters per sub-env\n",
    "        self._current_ep_rewards = [0.0] * n_envs\n",
    "        self._current_ep_counts = [1] * n_envs\n",
    "        self._current_step_in_episode = [0] * n_envs\n",
    "        super()._on_training_start()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        obs_vec = self.locals.get(\"new_obs\")\n",
    "        acts    = self.locals.get(\"actions\")\n",
    "        rews    = self.locals.get(\"rewards\")\n",
    "        dones   = self.locals.get(\"dones\")\n",
    "        step    = int(self.num_timesteps)\n",
    "\n",
    "        # log loss if present\n",
    "        loss_val = self.logger.name_to_value.get(\"train/loss\")\n",
    "        if loss_val is not None:\n",
    "            self.losses.append(float(loss_val))\n",
    "            self.loss_timesteps.append(step)\n",
    "\n",
    "        # iterate each sub-env\n",
    "        for idx, (obs, act, rew, done) in enumerate(zip(obs_vec, acts, rews, dones)):\n",
    "            # flatten observation\n",
    "            flat = obs.flatten().tolist()\n",
    "            # build row with metadata\n",
    "            row = {f\"x{i}\": flat[i] for i in range(len(flat))}\n",
    "            row.update({\n",
    "                \"env_id\": idx,\n",
    "                \"episode\": self._current_ep_counts[idx],\n",
    "                \"step_in_ep\": self._current_step_in_episode[idx],\n",
    "                \"action\": int(act),\n",
    "                \"reward\": float(rew),\n",
    "                \"global_step\": step\n",
    "            })\n",
    "            self.rows.append(row)\n",
    "\n",
    "            # accumulate per-episode reward\n",
    "            self._current_ep_rewards[idx] += float(rew)\n",
    "            # increment step in episode\n",
    "            self._current_step_in_episode[idx] += 1\n",
    "\n",
    "            # if end of episode for this env\n",
    "            if done:\n",
    "                # log reward and finalize episode\n",
    "                print(f\"Env {idx} Episode {self._current_ep_counts[idx]} done at global step {step}, total reward: {self._current_ep_rewards[idx]:.3f}\")\n",
    "                self.episode_rewards.append(self._current_ep_rewards[idx])\n",
    "                # reset for next episode\n",
    "                self._current_ep_rewards[idx] = 0.0\n",
    "                self._current_ep_counts[idx] += 1\n",
    "                self._current_step_in_episode[idx] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # build full-step DataFrame\n",
    "        self.df = pd.DataFrame(self.rows)\n",
    "        # build episodes summary DataFrame\n",
    "        self.ep_df = pd.DataFrame({\n",
    "            \"episode_global\": range(1, len(self.episode_rewards) + 1),\n",
    "            \"return\": self.episode_rewards\n",
    "        })\n",
    "        super()._on_training_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa10cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1\n",
      "Dataset '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1' copied to '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1/../../../../results/2025-08-27_15-43-41'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Monitor' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m env = Monitor(CityLearnDQNWrapper(cl_env, n_bins=\u001b[32m5\u001b[39m))\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Train env\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m train_env = \u001b[43mSubprocVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m train_env = VecMonitor(train_env)\n\u001b[32m     72\u001b[39m train_env = VecNormalize(train_env, norm_obs=\u001b[38;5;28;01mTrue\u001b[39;00m, norm_reward=\u001b[38;5;28;01mTrue\u001b[39;00m, clip_obs=\u001b[32m10.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hkfs/home/haicore/iai/cj9272/citylearn_env/lib64/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:106\u001b[39m, in \u001b[36mSubprocVecEnv.__init__\u001b[39m\u001b[34m(self, env_fns, start_method)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.waiting = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.closed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m n_envs = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv_fns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# Fork is not a thread safe method (see issue #217)\u001b[39;00m\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# but is more user friendly (does not require to wrap the code in\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# a `if __name__ == \"__main__\":`)\u001b[39;00m\n\u001b[32m    112\u001b[39m     forkserver_available = \u001b[33m\"\u001b[39m\u001b[33mforkserver\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mp.get_all_start_methods()\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'Monitor' has no len()"
     ]
    }
   ],
   "source": [
    "# Minimal DQN on standard CityLearnEnv (single building, single action)\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "\n",
    "# --- wrapper: single-agent SB3 env on top of CityLearnEnv + action discretization ---\n",
    "class CityLearnDQNWrapper(gym.Env):\n",
    "    \"\"\"Wrap standard CityLearnEnv for SB3 DQN:\n",
    "       - single central agent, we control building 0\n",
    "       - discretize the [-1, 1] storage action into n_bins\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, cl_env: CityLearnEnv, n_bins: int = 5):\n",
    "        super().__init__()\n",
    "        self.cl = cl_env\n",
    "        self.n_bins = int(n_bins)\n",
    "        assert self.n_bins >= 2, \"n_bins must be >= 2\"\n",
    "\n",
    "        # Build discrete action space\n",
    "        self.action_space = spaces.Discrete(self.n_bins)\n",
    "\n",
    "        # Infer observation shape (use building 0 obs)\n",
    "        self.cl.reset()\n",
    "        obs0 = np.array(self.cl.observations[0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=obs0.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Precompute action broadcast size (how many controls for bldg 0)\n",
    "        self.N = self.cl.action_space[0].shape[0]  # should be 1 if only electrical_storage is active\n",
    "\n",
    "    def _map_discrete_to_frac(self, a: int) -> float:\n",
    "        # map {0..n_bins-1} -> [-1, 1]\n",
    "        if self.n_bins == 1:\n",
    "            return 0.0\n",
    "        return -1.0 + 2.0 * (a / (self.n_bins - 1))\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.cl.reset(seed=seed)\n",
    "        obs = np.array(self.cl.observations[0], dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, a: int):\n",
    "        # map discrete to continuous fraction\n",
    "        frac = float(np.clip(self._map_discrete_to_frac(int(a)), -1.0, 1.0))\n",
    "        # build CityLearn action format: list per building -> flat list for building 0\n",
    "        actions = [[frac] * self.N]  # control building 0 only\n",
    "        obs_all, rewards, terminated, truncated, _info = self.cl.step(actions)\n",
    "\n",
    "        obs = np.array(obs_all[0], dtype=np.float32)\n",
    "        reward = float(rewards[0])\n",
    "        term = bool(terminated)\n",
    "        trunc = bool(truncated)\n",
    "        return obs, reward, term, trunc, {}\n",
    "\n",
    "# ----------------- build env -----------------\n",
    "\n",
    "cl_env = CityLearnEnv(schema, central_agent=True)\n",
    "env = Monitor(CityLearnDQNWrapper(cl_env, n_bins=5))\n",
    "\n",
    "# Train env\n",
    "train_env = SubprocVecEnv(env)\n",
    "train_env = VecMonitor(train_env)\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# New eval env\n",
    "eval_env = DummyVecEnv([lambda: Monitor(CityLearnEnv(building_df=bld, pricing_df=prc, weather_df=wth, carbon_df=car, n_bins=5))])\n",
    "eval_env = VecMonitor(eval_env) \n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "\n",
    "# Evaluate every 2k steps over 5 episodes\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    log_path=\"logs/eval/\",\n",
    "    best_model_save_path=\"logs/best_model/\",\n",
    "    #eval_freq=len(building_data), # run evaluation every 5k timesteps, TODO: change to 5_000\n",
    "    eval_freq=1000, # run evaluation every n timesteps, TODO: change n to 5_000\n",
    "    n_eval_episodes=5,       # average over 5 full episodes\n",
    "    deterministic=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_callback = TrainLoggerCallback()\n",
    "\n",
    "# ----------------- train DQN (super simple) -----------------\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=256,\n",
    "    learning_starts=2_000,\n",
    "    train_freq=256,\n",
    "    target_update_interval=1_000,\n",
    "    gamma=0.98,\n",
    "    exploration_fraction=0.3,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    ")\n",
    "\n",
    "# Model training\n",
    "#model.learn(total_timesteps=100_000)\n",
    "T = len(building_data) # 1 episode = 2208 timesteps\n",
    "num_episodes = 10 # 10 episodes\n",
    "model.learn(\n",
    "    total_timesteps=num_episodes * T,\n",
    "    callback=CallbackList([train_callback, eval_callback])\n",
    ")\n",
    "\n",
    "print(\"Eval timesteps:\", eval_callback.evaluations_timesteps)\n",
    "print(\"Eval results   :\", eval_callback.evaluations_results)\n",
    "\n",
    "\n",
    "print(\"---------------Train callback: \\n\", train_callback.df)\n",
    "print(\"---------------Episode rewards: \\n\", train_callback.ep_df)\n",
    "\n",
    "\n",
    "# Evaluation results\n",
    "all_rewards = eval_callback.evaluations_results # List of lists: each element are rewards from an eval round\n",
    "eval_steps  = eval_callback.evaluations_timesteps # timesteps at which the evaluations were run\n",
    "\n",
    "# Mean reward per round\n",
    "mean_rewards = [np.mean(r) for r in all_rewards]\n",
    "\n",
    "# ----------------- quick evaluation (one episode) -----------------\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "ret = 0.0\n",
    "while not done:\n",
    "    action = model.predict(obs, deterministic=True)[0]\n",
    "    obs, r, term, trunc, _ = env.step(action)\n",
    "    ret += r\n",
    "    done = term or trunc\n",
    "print(f\"Episode return: {ret:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (citylearn_env)",
   "language": "python",
   "name": "citylearn_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
