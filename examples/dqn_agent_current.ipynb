{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81f853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 16:42:00.434513: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-27 16:42:00.458205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756305720.486376  341288 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756305720.494615  341288 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756305720.516043  341288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756305720.516078  341288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756305720.516081  341288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756305720.516084  341288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-27 16:42:00.523083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.7\n",
      "Torch : 2.7.0+cu126\n",
      "CityLearn: 2.4.1\n",
      "Optuna installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/hkfs/home/haicore/iai/cj9272/citylearn_env/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "# System operations\n",
    "import os\n",
    "\n",
    "# Type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import torch as th\n",
    "\n",
    "# CityLearn\n",
    "import citylearn\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.wrappers import NormalizedObservationWrapper, StableBaselines3Wrapper\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "# Baseline RL algorithms\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Utils\n",
    "from utils.env_utils import DiscretizeActionWrapper, keep_only_electrical_storage, keep_only_core_observations\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Torch :', th.__version__)\n",
    "print('CityLearn:', citylearn.__version__)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"optuna\", \"optuna-dashboard\"])\n",
    "print(\"Optuna installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4530515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2023_phase_3_1'  # adjust if needed\n",
    "ROOT_DIR = r'/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1'\n",
    "REWARD_FN = {  # CostReward \n",
    "    'type': 'citylearn.reward_function.CostReward',\n",
    "    'attributes': {}\n",
    "}\n",
    "PRICING_FILE = 'pricing_germany_2023_june_to_august.csv'\n",
    "ACTION_LABELS = ['100%_discharge','50%_discharge','idle','50%_charge','100%_charge']\n",
    "NAME_TO_FRAC = {\n",
    "    '100%_discharge': -1.0,\n",
    "    '50%_discharge': -0.5,\n",
    "    'idle': 0.0,\n",
    "    '50%_charge': 0.5,\n",
    "    '100%_charge': 1.0,\n",
    "}\n",
    "EVAL_EPISODES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcff422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [\n",
    "    # building_df\n",
    "    'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "    'indoor_dry_bulb_temperature',\n",
    "    'average_unmet_cooling_setpoint_difference',\n",
    "    'indoor_relative_humidity',\n",
    "    'non_shiftable_load', 'dhw_demand',\n",
    "    'cooling_demand', 'heating_demand',\n",
    "    'solar_generation', 'occupant_count',\n",
    "    'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "    'indoor_dry_bulb_temperature_heating_set_point', 'hvac_mode',\n",
    "    # weather_df\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance',\n",
    "    'outdoor_dry_bulb_temperature_predicted_1',\n",
    "    'outdoor_dry_bulb_temperature_predicted_2',\n",
    "    'outdoor_dry_bulb_temperature_predicted_3',\n",
    "    'outdoor_relative_humidity_predicted_1',\n",
    "    'outdoor_relative_humidity_predicted_2',\n",
    "    'outdoor_relative_humidity_predicted_3',\n",
    "    'diffuse_solar_irradiance_predicted_1',\n",
    "    'diffuse_solar_irradiance_predicted_2',\n",
    "    'diffuse_solar_irradiance_predicted_3',\n",
    "    'direct_solar_irradiance_predicted_1',\n",
    "    'direct_solar_irradiance_predicted_2',\n",
    "    'direct_solar_irradiance_predicted_3',\n",
    "    # carbon_df \n",
    "    'carbon_intensity',\n",
    "    # pricing_df\n",
    "    'electricity_pricing',\n",
    "    'electricity_pricing_predicted_1',\n",
    "    'electricity_pricing_predicted_2',\n",
    "    'electricity_pricing_predicted_3'\n",
    "]\n",
    "bld_cols = [\n",
    "            'month', 'hour', 'day_type', 'daylight_savings_status',\n",
    "            'indoor_dry_bulb_temperature',\n",
    "            'average_unmet_cooling_setpoint_difference',\n",
    "            'indoor_relative_humidity', 'non_shiftable_load',\n",
    "            'dhw_demand', 'cooling_demand', 'heating_demand',\n",
    "            'solar_generation', 'occupant_count',\n",
    "            'indoor_dry_bulb_temperature_cooling_set_point',\n",
    "            'indoor_dry_bulb_temperature_heating_set_point',\n",
    "            'hvac_mode'\n",
    "]\n",
    "wth_cols = [\n",
    "    'outdoor_dry_bulb_temperature',\n",
    "    'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance',\n",
    "    'direct_solar_irradiance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b053b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Go here /home/iai/cj9272/.cache/citylearn/v2.4.1/datasets/citylearn_challenge_2023_phase_3_1/schema.json \n"
     ]
    }
   ],
   "source": [
    "# --- Load schema ---\n",
    "dataset = DataSet()\n",
    "schema = dataset.get_schema(DATASET_NAME)\n",
    "schema['root_directory'] = ROOT_DIR\n",
    "schema['reward_function'] = REWARD_FN\n",
    "\n",
    "price_file = PRICING_FILE # Set pricing file\n",
    "if 'buildings' not in schema:\n",
    "    raise RuntimeError(\"schema does not contain 'buildings' (make sure schema is loaded first)\")\n",
    "for bname, bconf in schema['buildings'].items():\n",
    "    bconf['pricing'] = price_file\n",
    "\n",
    "schema = keep_only_electrical_storage(schema) # Activate only the electrical storage control (fix \"Expected 18 actions but got 1\")\n",
    "schema = keep_only_core_observations(schema, extra_keep=['carbon_intensity', 'non_shiftable_load'], drop_predictions=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec41d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoggerCallback(BaseCallback):\n",
    "    \"\"\"Logging State, Action, Reward per step and Loss per update phase.\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Will collect a dict per env-step\n",
    "        self.rows = []\n",
    "        # Loss values and their timesteps (global)\n",
    "        self.losses = []\n",
    "        self.loss_timesteps = []\n",
    "        # Completed episode returns (global list)\n",
    "        self.episode_rewards = []\n",
    "        # Placeholders for per-env tracking\n",
    "        self._current_ep_rewards = []         # sum of rewards in current episode per env\n",
    "        self._current_ep_counts = []          # episode index per env\n",
    "        self._current_step_in_episode = []    # step counter (0..T-1) per env\n",
    "\n",
    "        # DataFrames to populate at end\n",
    "        self.df = pd.DataFrame()\n",
    "        self.ep_df = pd.DataFrame()\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        try:\n",
    "            n_envs = self.training_env.num_envs\n",
    "        except AttributeError:\n",
    "            n_envs = 1\n",
    "        # initialize counters per sub-env\n",
    "        self._current_ep_rewards = [0.0] * n_envs\n",
    "        self._current_ep_counts = [1] * n_envs\n",
    "        self._current_step_in_episode = [0] * n_envs\n",
    "        super()._on_training_start()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        obs_vec = self.locals.get(\"new_obs\")\n",
    "        acts    = self.locals.get(\"actions\")\n",
    "        rews    = self.locals.get(\"rewards\")\n",
    "        dones   = self.locals.get(\"dones\")\n",
    "        step    = int(self.num_timesteps)\n",
    "\n",
    "        # log loss if present\n",
    "        loss_val = self.logger.name_to_value.get(\"train/loss\")\n",
    "        if loss_val is not None:\n",
    "            self.losses.append(float(loss_val))\n",
    "            self.loss_timesteps.append(step)\n",
    "\n",
    "        # iterate each sub-env\n",
    "        for idx, (obs, act, rew, done) in enumerate(zip(obs_vec, acts, rews, dones)):\n",
    "            # flatten observation\n",
    "            flat = obs.flatten().tolist()\n",
    "            # build row with metadata\n",
    "            row = {f\"x{i}\": flat[i] for i in range(len(flat))}\n",
    "            row.update({\n",
    "                \"env_id\": idx,\n",
    "                \"episode\": self._current_ep_counts[idx],\n",
    "                \"step_in_ep\": self._current_step_in_episode[idx],\n",
    "                \"action\": int(act),\n",
    "                \"reward\": float(rew),\n",
    "                \"global_step\": step\n",
    "            })\n",
    "            row['action_name'] = ACTION_LABELS[int(act)]\n",
    "            row['action_frac'] = NAME_TO_FRAC[row['action_name']]\n",
    "            self.rows.append(row)\n",
    "\n",
    "            # accumulate per-episode reward\n",
    "            self._current_ep_rewards[idx] += float(rew)\n",
    "            # increment step in episode\n",
    "            self._current_step_in_episode[idx] += 1\n",
    "\n",
    "            # if end of episode for this env\n",
    "            if done:\n",
    "                # log reward and finalize episode\n",
    "                print(f\"Env {idx} Episode {self._current_ep_counts[idx]} done at global step {step}, total reward: {self._current_ep_rewards[idx]:.3f}\")\n",
    "                self.episode_rewards.append(self._current_ep_rewards[idx])\n",
    "                # reset for next episode\n",
    "                self._current_ep_rewards[idx] = 0.0\n",
    "                self._current_ep_counts[idx] += 1\n",
    "                self._current_step_in_episode[idx] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # build full-step DataFrame\n",
    "        self.df = pd.DataFrame(self.rows)\n",
    "        # build episodes summary DataFrame\n",
    "        self.ep_df = pd.DataFrame({\n",
    "            \"episode_global\": range(1, len(self.episode_rewards) + 1),\n",
    "            \"return\": self.episode_rewards\n",
    "        })\n",
    "        super()._on_training_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d722e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1\n",
      "Dataset '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1' copied to '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1/../../../../results/2025-08-27_16-42-07'\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import copy, os, time, json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.wrappers import NormalizedObservationWrapper, StableBaselines3Wrapper\n",
    "\n",
    "# Keep only one building: 1-D action\n",
    "one_building = \"Building_1\"\n",
    "schema_1b = dict(schema)  # shallow copy\n",
    "schema_1b['buildings'] = {k: v for k, v in schema['buildings'].items() if k == one_building}\n",
    "if not schema_1b['buildings']:\n",
    "    raise RuntimeError(f\"{one_building} not found in schema['buildings']\")\n",
    "\n",
    "train_env = CityLearnEnv(schema_1b, central_agent=True)\n",
    "train_env = NormalizedObservationWrapper(train_env)\n",
    "train_env = StableBaselines3Wrapper(train_env)\n",
    "# Discretize action space for DQN:\n",
    "train_env = DiscretizeActionWrapper(train_env, n_bins=5)\n",
    "train_env = Monitor(train_env)\n",
    "\n",
    "train_callback = TrainLoggerCallback()\n",
    "\n",
    "T = train_env.unwrapped.time_steps  # 2208\n",
    "num_episodes = 10\n",
    "TOTAL_TIMESTEPS = num_episodes * T\n",
    "\n",
    "model = DQN(\n",
    "    policy='MlpPolicy',\n",
    "    env=train_env,\n",
    "    seed=0,\n",
    "    #learning_starts=1000,\n",
    "    #learning_rate=3e-4,\n",
    "    verbose=1, # logging: info\n",
    "    policy_kwargs=dict(net_arch=[256, 256, 256], activation_fn=th.nn.ReLU),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 0 Episode 1 done at global step 2207, total reward: -287.480\n",
      "Env 0 Episode 2 done at global step 4414, total reward: -266.891\n",
      "Env 0 Episode 3 done at global step 6621, total reward: -266.357\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=train_callback)\n",
    "print(f\"Total time elapsed for {TOTAL_TIMESTEPS} steps: {(time.time()-start_time):.2f} seconds\")\n",
    "model.save(\"dqn_01\")\n",
    "try:\n",
    "    model.save_replay_buffer(\"dqn_01_replaybuffer.pkl\")\n",
    "except Exception as e:\n",
    "    print(\"Replay buffer not saved:\", e)\n",
    "print(\"---------------Train callback: \\n\", train_callback.df)\n",
    "print(\"---------------Episode rewards: \\n\", train_callback.ep_df)\n",
    "# Save training logs\n",
    "train_callback.df.to_csv(\"train_steps.csv\", index=False)\n",
    "train_callback.ep_df.to_csv(\"train_episodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12693856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1\n",
      "Dataset '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1' copied to '/hkfs/home/haicore/iai/cj9272/Bachelorthesis_DQN_Agent/data/datasets/citylearn_challenge_2023_phase_3_1/../../../../results/2025-08-27_13-03-06'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "schema_eval = copy.deepcopy(schema_1b)\n",
    "eval_env = CityLearnEnv(schema_eval, central_agent=True)\n",
    "eval_env = NormalizedObservationWrapper(eval_env)\n",
    "eval_env = StableBaselines3Wrapper(eval_env)\n",
    "eval_env = DiscretizeActionWrapper(eval_env, n_bins=5)\n",
    "\n",
    "model = DQN.load(\"dqn_01\", env=eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fa494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval episodes: 10 total steps: 22070\n"
     ]
    }
   ],
   "source": [
    "# Eval: collect discrete actions and rewards per step (overwrites simple eval_dqn)\n",
    "def eval_dqn(model, eval_env, schema, episodes=3, deterministic=True):\n",
    "    \"\"\"Deterministic evaluation that returns actions/rewards for plotting.\"\"\"\n",
    "    ep_lengths, actions_disc_list, step_rewards_all = [], [], []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = eval_env.reset()\n",
    "        done = False\n",
    "        ep_actions, ep_rewards = [], []\n",
    "        while not done:\n",
    "            act_disc, _ = model.predict(obs, deterministic=deterministic)\n",
    "            obs, r, terminated, truncated, _ = eval_env.step(act_disc)\n",
    "            done = bool(terminated or truncated)\n",
    "            ep_actions.append(int(act_disc))\n",
    "            ep_rewards.append(float(r))\n",
    "        ep_lengths.append(len(ep_rewards))\n",
    "        actions_disc_list.append(np.array(ep_actions, dtype=int))\n",
    "        step_rewards_all.extend(ep_rewards)\n",
    "\n",
    "    # Pricing for Building_1 \n",
    "    building_name = 'Building_1'\n",
    "    price_file = schema['buildings'][building_name]['pricing']\n",
    "    price_path = os.path.join(ROOT_DIR, price_file)\n",
    "    prc = pd.read_csv(price_path)\n",
    "    T = len(prc)\n",
    "\n",
    "    return {\n",
    "        'ep_lengths': np.array(ep_lengths, dtype=int),\n",
    "        'actions_disc_list': actions_disc_list,\n",
    "        'step_rewards': np.array(step_rewards_all, dtype=float),\n",
    "        'price_df': prc,\n",
    "        'episode_len': T,\n",
    "        'building_name': building_name,\n",
    "    }\n",
    "\n",
    "# Run eval and store results\n",
    "eval_results = eval_dqn(model, eval_env, schema_eval, episodes=10, deterministic=True)\n",
    "print('Eval episodes:', len(eval_results['ep_lengths']), 'total steps:', len(eval_results['step_rewards']))\n",
    "\n",
    "# Save eval results\n",
    "eval_path = \"eval_results.pkl\"\n",
    "with open(eval_path, \"wb\") as f:\n",
    "    pickle.dump(eval_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bceb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-27 13:23:07,751] Using an existing study with name 'dqn_citylearn' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study ready: dqn_citylearn sqlite:////hkfs/home/haicore/iai/cj9272/artifacts/optuna_dqn.db\n"
     ]
    }
   ],
   "source": [
    "# Optuna objective for DQN on CityLearn (single building, same wrappers)\n",
    "import os, copy, json\n",
    "import numpy as np\n",
    "import optuna\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Reuse your DiscretizeActionWrapper, NormalizedObservationWrapper, StableBaselines3Wrapper, CityLearnEnv\n",
    "# Assumes 'schema', 'ROOT_DIR', 'one_building' already exist from your previous cells.\n",
    "\n",
    "ART_DIR = os.path.join(os.getcwd(), \"artifacts\")\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "STORAGE_URL = f\"sqlite:///{os.path.join(ART_DIR, 'optuna_dqn.db')}\"\n",
    "STUDY_NAME  = \"dqn_citylearn\"\n",
    "\n",
    "def make_env_one_building(schema_src, building_name=\"Building_1\", n_bins=5, monitor=False):\n",
    "    schema_1b = copy.deepcopy(schema_src)\n",
    "    schema_1b['buildings'] = {k: v for k, v in schema_src['buildings'].items() if k == building_name}\n",
    "    if not schema_1b['buildings']:\n",
    "        raise RuntimeError(f\"{building_name} not found in schema['buildings']\")\n",
    "    env = CityLearnEnv(schema_1b, central_agent=True)\n",
    "    env = NormalizedObservationWrapper(env)\n",
    "    env = StableBaselines3Wrapper(env)\n",
    "    env = DiscretizeActionWrapper(env, n_bins=n_bins)\n",
    "    if monitor:\n",
    "        env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "def evaluate_mean_reward(model, env, episodes=2, deterministic=True):\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        while not done:\n",
    "            act, _ = model.predict(obs, deterministic=deterministic)\n",
    "            obs, r, terminated, truncated, _ = env.step(act)\n",
    "            ep_ret += float(r)\n",
    "            done = bool(terminated or truncated)\n",
    "        rewards.append(ep_ret)\n",
    "    return float(np.mean(rewards))\n",
    "\n",
    "class OptunaEvalPruningCallback:\n",
    "    def __init__(self, trial, model, eval_env, eval_every_steps, eval_episodes=2):\n",
    "        self.trial = trial\n",
    "        self.model = model\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_every_steps = int(eval_every_steps)\n",
    "        self.eval_episodes = int(eval_episodes)\n",
    "        self._last_step = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def __call__(self, locals_, globals_):\n",
    "        # Called by SB3 when using callback= in learn (old-style callable)\n",
    "        step = int(locals_.get(\"self\").num_timesteps)\n",
    "        if step - self._last_step >= self.eval_every_steps:\n",
    "            self._last_step = step\n",
    "            mean_r = evaluate_mean_reward(self.model, self.eval_env, episodes=self.eval_episodes, deterministic=True)\n",
    "            self.trial.report(mean_r, step=step)\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False  # stop training\n",
    "        return True\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Search space\n",
    "    lr      = trial.suggest_float(\"learning_rate\", 1e-5, 3e-3, log=True)\n",
    "    gamma   = trial.suggest_float(\"gamma\", 0.90, 0.9999)\n",
    "    buffer  = trial.suggest_int(\"buffer_size\", 50_000, 200_000, step=25_000)\n",
    "    batch   = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    tau     = trial.suggest_float(\"tau\", 0.005, 1.0, log=True)\n",
    "    tgt_upd = trial.suggest_int(\"target_update_interval\", 250, 5000, step=250)\n",
    "    train_f = trial.suggest_int(\"train_freq\", 1, 8)\n",
    "    grad_st = trial.suggest_int(\"gradient_steps\", 1, 4)\n",
    "    expl_fr = trial.suggest_float(\"exploration_fraction\", 0.05, 0.4)\n",
    "    expl_fin= trial.suggest_float(\"exploration_final_eps\", 0.01, 0.1)\n",
    "    starts  = trial.suggest_int(\"learning_starts\", 500, 5000, step=500)\n",
    "    arch    = trial.suggest_categorical(\"net_arch\", [(256,256), (256,256,256), (512,512)])\n",
    "\n",
    "    # Build train/eval envs\n",
    "    train_env = make_env_one_building(schema, building_name=\"Building_1\", n_bins=5, monitor=True)\n",
    "    eval_env  = make_env_one_building(schema, building_name=\"Building_1\", n_bins=5, monitor=False)\n",
    "    T = train_env.unwrapped.time_steps\n",
    "\n",
    "    # Episodes per trial (keep small for speed)\n",
    "    episodes_per_trial = 3\n",
    "    total_steps = episodes_per_trial * T\n",
    "\n",
    "    model = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        seed=0,\n",
    "        learning_rate=lr,\n",
    "        gamma=gamma,\n",
    "        buffer_size=buffer,\n",
    "        batch_size=batch,\n",
    "        tau=tau,\n",
    "        target_update_interval=tgt_upd,\n",
    "        train_freq=train_f,\n",
    "        gradient_steps=grad_st,\n",
    "        exploration_fraction=expl_fr,\n",
    "        exploration_final_eps=expl_fin,\n",
    "        learning_starts=starts,\n",
    "        verbose=0,\n",
    "        policy_kwargs=dict(net_arch=list(arch), activation_fn=th.nn.ReLU),\n",
    "        device=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Evaluation + pruning during training\n",
    "    cb = OptunaEvalPruningCallback(trial, model, eval_env, eval_every_steps=T//2, eval_episodes=2)\n",
    "    try:\n",
    "        model.learn(total_timesteps=total_steps, callback=cb)\n",
    "        if cb.is_pruned:\n",
    "            raise optuna.TrialPruned()\n",
    "    finally:\n",
    "        # Ensure envs are closed\n",
    "        try: train_env.close()\n",
    "        except: pass\n",
    "        try: eval_env.close()\n",
    "        except: pass\n",
    "\n",
    "    # Final score\n",
    "    eval_env = make_env_one_building(schema, building_name=\"Building_1\", n_bins=5, monitor=False)\n",
    "    score = evaluate_mean_reward(model, eval_env, episodes=3, deterministic=True)\n",
    "    eval_env.close()\n",
    "\n",
    "    # Save best-so-far model snapshot\n",
    "    trial.set_user_attr(\"final_mean_reward\", score)\n",
    "    return score\n",
    "\n",
    "# Create/continue a study\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    direction=\"maximize\",\n",
    "    storage=STORAGE_URL,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(seed=0),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1), # NopPruner()\n",
    ")\n",
    "print(\"Study ready:\", STUDY_NAME, STORAGE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna hyperparameter optimization\n",
    "\n",
    "N_TRIALS = 3  # increase as needed\n",
    "#study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
    "#print(\"Done. Trials now:\", len(study.trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd6ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    136\u001b[39m     eval_results = eval_dqn_quick(model, eval_env_local, schema_eval_local, episodes=\u001b[32m5\u001b[39m, deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_results\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m er = \u001b[43m_ensure_eval_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m actions_disc = np.concatenate(er[\u001b[33m'\u001b[39m\u001b[33mactions_disc_list\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[32m0\u001b[39m)\n\u001b[32m    141\u001b[39m actions_frac = pd.Series(actions_disc).map({\u001b[32m0\u001b[39m:-\u001b[32m1.0\u001b[39m, \u001b[32m1\u001b[39m:-\u001b[32m0.5\u001b[39m, \u001b[32m2\u001b[39m:\u001b[32m0.0\u001b[39m, \u001b[32m3\u001b[39m:\u001b[32m0.5\u001b[39m, \u001b[32m4\u001b[39m:\u001b[32m1.0\u001b[39m}).to_numpy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36m_ensure_eval_results\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    120\u001b[39m     prc = pd.read_csv(price_path)\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    122\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mep_lengths\u001b[39m\u001b[33m'\u001b[39m: np.array(ep_lengths, dtype=\u001b[38;5;28mint\u001b[39m),\n\u001b[32m    123\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mactions_disc_list\u001b[39m\u001b[33m'\u001b[39m: actions_disc_list,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbuilding_name\u001b[39m\u001b[33m'\u001b[39m: bname,\n\u001b[32m    128\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m schema_eval_local = copy.deepcopy(schema_1b) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mschema_1b\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m copy.deepcopy(\u001b[43mschema\u001b[49m)\n\u001b[32m    131\u001b[39m eval_env_local = CityLearnEnv(schema_eval_local, central_agent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    132\u001b[39m eval_env_local = NormalizedObservationWrapper(eval_env_local)\n",
      "\u001b[31mNameError\u001b[39m: name 'schema' is not defined"
     ]
    }
   ],
   "source": [
    "# 6 DQN plots matching the PPO ones\n",
    "\n",
    "import os, copy, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "# Ensure INT_TO_FRAC and ACTION_LABELS exist\n",
    "INT_TO_FRAC = np.array([-1.0, -0.5, 0.0, 0.5, 1.0], dtype=np.float32)\n",
    "ACTION_LABELS = ['100%_discharge','50%_discharge','idle','50%_charge','100%_charge']\n",
    "\n",
    "# ------------- Helpers -------------\n",
    "def unwrap_to_citylearn(env):\n",
    "    cur, seen = env, set()\n",
    "    while cur is not None and id(cur) not in seen:\n",
    "        seen.add(id(cur))\n",
    "        if isinstance(cur, CityLearnEnv):\n",
    "            return cur\n",
    "        cur = getattr(cur, \"env\", getattr(cur, \"unwrapped\", None))\n",
    "    raise RuntimeError(\"CityLearnEnv not found inside wrappers.\")\n",
    "\n",
    "def capture_ep_series_dqn(model, env, building_name=\"Building_1\", deterministic=True, max_steps=None):\n",
    "    \"\"\"Run one episode (or first max_steps) and capture per-step action, reward, and base env series.\"\"\"\n",
    "    base = unwrap_to_citylearn(env)\n",
    "\n",
    "    # Building index\n",
    "    bld_names = [b.name for b in base.buildings]\n",
    "    if building_name not in bld_names:\n",
    "        building_name = bld_names[0]\n",
    "    b_idx = bld_names.index(building_name)\n",
    "\n",
    "    # Observation indices\n",
    "    obs_names = getattr(base, 'observation_names', None)\n",
    "    if obs_names is None:\n",
    "        raise AttributeError(\"CityLearnEnv has no 'observation_names'.\")\n",
    "    obs_b = obs_names[b_idx]\n",
    "    name_to_idx = {n: i for i, n in enumerate(obs_b)}\n",
    "\n",
    "    i_net = name_to_idx.get('net_electricity_consumption')\n",
    "    if i_net is None:\n",
    "        i_net = name_to_idx.get('net_electricity_consumption_without_storage')\n",
    "    i_nsl = name_to_idx.get('non_shiftable_load')\n",
    "    i_price = name_to_idx.get('electricity_pricing')\n",
    "    i_solar = name_to_idx.get('solar_generation')\n",
    "\n",
    "    # Run rollout\n",
    "    obs, _ = env.reset()\n",
    "    done, steps = False, 0\n",
    "    t, a_id_list, a_frac_list, rew_list = [], [], [], []\n",
    "    net_true, nsl_true, price_true, solar_true = [], [], [], []\n",
    "\n",
    "    while not done and (max_steps is None or steps < max_steps):\n",
    "        act, _ = model.predict(obs, deterministic=deterministic)\n",
    "\n",
    "        # Discrete action id -> fraction\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            a_id = int(act)\n",
    "        elif isinstance(env.action_space, gym.spaces.MultiDiscrete):\n",
    "            a_id = int(np.asarray(act).reshape(-1)[b_idx])\n",
    "        else:\n",
    "            val = float(np.asarray(act).reshape(-1)[b_idx])\n",
    "            a_id = int(np.argmin(np.abs(INT_TO_FRAC - val)))\n",
    "        a_frac = float(INT_TO_FRAC[a_id])\n",
    "\n",
    "        obs, r, terminated, truncated, _ = env.step(act)\n",
    "        done = bool(terminated or truncated)\n",
    "\n",
    "        raw = base.observations[b_idx]\n",
    "        t.append(steps)\n",
    "        a_id_list.append(a_id)\n",
    "        a_frac_list.append(a_frac)\n",
    "        rew_list.append(float(r))\n",
    "        net_true.append(float(raw[i_net]) if i_net is not None else np.nan)\n",
    "        nsl_true.append(float(raw[i_nsl]) if i_nsl is not None else np.nan)\n",
    "        price_true.append(float(raw[i_price]) if i_price is not None else np.nan)\n",
    "        solar_true.append(float(raw[i_solar]) if i_solar is not None else np.nan)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        't': np.arange(len(t)),\n",
    "        'action_id': a_id_list,\n",
    "        'action_frac': a_frac_list,\n",
    "        'reward': rew_list,\n",
    "        'net_load': net_true,\n",
    "        'non_shiftable_load': nsl_true,\n",
    "        'price': price_true,\n",
    "        'solar_generation': solar_true,\n",
    "    })\n",
    "    return df, building_name\n",
    "\n",
    "# ------------- Plot 1 + 2: Actions over time + distribution -------------\n",
    "# Use cached eval_results if available, else run a quick eval\n",
    "def _ensure_eval_results():\n",
    "    global eval_results\n",
    "    if 'eval_results' in globals():\n",
    "        return eval_results\n",
    "    # Fallback quick eval (deterministic)\n",
    "    def eval_dqn_quick(model, eval_env, schema, episodes=3, deterministic=True):\n",
    "        ep_lengths, actions_disc_list, step_rewards_all = [], [], []\n",
    "        for _ in range(episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            done = False\n",
    "            ep_actions, ep_rewards = [], []\n",
    "            while not done:\n",
    "                act_disc, _ = model.predict(obs, deterministic=deterministic)\n",
    "                obs, r, terminated, truncated, _ = eval_env.step(act_disc)\n",
    "                done = bool(terminated or truncated)\n",
    "                ep_actions.append(int(act_disc))\n",
    "                ep_rewards.append(float(r))\n",
    "            ep_lengths.append(len(ep_rewards))\n",
    "            actions_disc_list.append(np.array(ep_actions, dtype=int))\n",
    "            step_rewards_all.extend(ep_rewards)\n",
    "        # Pricing for Building_1\n",
    "        bname = 'Building_1'\n",
    "        price_file = schema['buildings'][bname]['pricing']\n",
    "        price_path = os.path.join(ROOT_DIR, price_file)\n",
    "        prc = pd.read_csv(price_path)\n",
    "        return {\n",
    "            'ep_lengths': np.array(ep_lengths, dtype=int),\n",
    "            'actions_disc_list': actions_disc_list,\n",
    "            'step_rewards': np.array(step_rewards_all, dtype=float),\n",
    "            'price_df': prc,\n",
    "            'episode_len': len(prc),\n",
    "            'building_name': bname,\n",
    "        }\n",
    "\n",
    "    schema_eval_local = copy.deepcopy(schema_1b) if 'schema_1b' in globals() else copy.deepcopy(schema)\n",
    "    eval_env_local = CityLearnEnv(schema_eval_local, central_agent=True)\n",
    "    eval_env_local = NormalizedObservationWrapper(eval_env_local)\n",
    "    eval_env_local = StableBaselines3Wrapper(eval_env_local)\n",
    "    eval_env_local = DiscretizeActionWrapper(eval_env_local, n_bins=5)\n",
    "\n",
    "    eval_results = eval_dqn_quick(model, eval_env_local, schema_eval_local, episodes=5, deterministic=True)\n",
    "    return eval_results\n",
    "\n",
    "er = _ensure_eval_results()\n",
    "actions_disc = np.concatenate(er['actions_disc_list'], axis=0)\n",
    "actions_frac = pd.Series(actions_disc).map({0:-1.0, 1:-0.5, 2:0.0, 3:0.5, 4:1.0}).to_numpy()\n",
    "hour_index = np.arange(len(actions_frac))\n",
    "building_name = er['building_name']\n",
    "\n",
    "# Plot 1: Actions over time (raw + MA)\n",
    "window = 500\n",
    "act_ma = pd.Series(actions_frac).rolling(window, min_periods=1).mean().to_numpy()\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.scatter(hour_index, actions_frac, s=5, alpha=0.35, label='raw')\n",
    "plt.plot(hour_index, act_ma, color='C2', lw=2, label=f'ma {window}')\n",
    "plt.title(f'DQN Eval Actions over time (Raw + MA) - {building_name}')\n",
    "plt.xlabel('Timestep'); plt.ylabel('Action [-1..1]'); plt.grid(True); plt.legend(loc='upper right')\n",
    "plt.yticks(INT_TO_FRAC.tolist(), ACTION_LABELS)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot 2: Action Distribution (5-bin)\n",
    "counts = pd.Series(actions_disc).value_counts().reindex(range(5)).fillna(0).astype(int)\n",
    "plt.figure(figsize=(4,3))\n",
    "ax = counts.plot(kind='bar', color=['C0','C1','C2','C3','C4'])\n",
    "ax.set_xticklabels(ACTION_LABELS, rotation=45, ha='right')\n",
    "total = counts.sum()\n",
    "ax.bar_label(ax.containers[0], labels=[f'{int(v)}\\n({v/total:.1%})' for v in counts.values])\n",
    "ax.set_ylim(0, counts.max()*1.5)\n",
    "plt.ylabel('Count'); plt.title('Action Distribution'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------- Plot 3: First 20 steps — actions vs loads -------------\n",
    "# ------------- Plots 4,5,6: Reward vs Price/Solar/Net (episode 1) -------------\n",
    "\n",
    "def plot_first_30_steps_actions_vs_loads_dqn(model, env, building_name=\"Building_1\", deterministic=True):\n",
    "    df30, bname = capture_ep_series_dqn(model, env, building_name, deterministic=deterministic, max_steps=30)\n",
    "\n",
    "    t = df30['t'].to_numpy()\n",
    "    net = df30['net_load'].to_numpy()\n",
    "    nsl = df30['non_shiftable_load'].to_numpy()\n",
    "    act = df30['action_frac'].to_numpy()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 3))\n",
    "    ax1.plot(t, net, color='C4', marker='o', label='Net Load (with storage)')\n",
    "    ax1.plot(t, nsl, color='C1', ls='--', marker='x', label='Non-shiftable Load (baseline)')\n",
    "    ax1.set_xlabel('Timestep')\n",
    "    ax1.set_xticks(t[::2])\n",
    "    ax1.set_ylabel('Load')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left', frameon=False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.step(t, act, where='mid', color='C0', label='Action')\n",
    "    ax2.set_ylabel('Action')\n",
    "    ax2.set_ylim(-1.1, 1.1)\n",
    "    ax2.set_yticks(INT_TO_FRAC.tolist())\n",
    "    ax2.set_yticklabels(ACTION_LABELS)\n",
    "\n",
    "    plt.title(f'DQN — first 30 steps — {bname} (deterministic={deterministic})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def dqn_ep1_reward_vs_series(model, env, building_name=\"Building_1\", deterministic=True):\n",
    "    df, bname = capture_ep_series_dqn(model, env, building_name, deterministic=deterministic, max_steps=None)\n",
    "\n",
    "    window_reward = 200\n",
    "    window_x = 24\n",
    "    t = df['t'].to_numpy()\n",
    "    reward_ma = pd.Series(df['reward']).rolling(window_reward, min_periods=1).mean().to_numpy()\n",
    "    price_ma  = pd.Series(df['price']).rolling(window_x, min_periods=1).mean().to_numpy()\n",
    "    solar_ma  = pd.Series(df['solar_generation']).rolling(window_x, min_periods=1).mean().to_numpy()\n",
    "    net_ma    = pd.Series(df['net_load']).rolling(window_x, min_periods=1).mean().to_numpy()\n",
    "\n",
    "\n",
    "    # 4) Reward vs Net Load\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 3.5))\n",
    "    ax1.plot(t, reward_ma, color='C3', label=f'Reward MA (w={window_reward})')\n",
    "    ax1.set_xlabel('Timestep'); ax1.set_ylabel(f'Reward MA ({window_reward})', color='C3')\n",
    "    ax1.tick_params(axis='y', labelcolor='C3'); ax1.grid(True, axis='x', alpha=0.3)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, net_ma, color='C4', label=f'Net Load MA (w={window_x})')\n",
    "    ax2.set_ylabel('Net Electricity Consumption', color='C4'); ax2.tick_params(axis='y', labelcolor='C4')\n",
    "    lines = ax1.get_lines() + ax2.get_lines()\n",
    "    ax1.legend(lines, [l.get_label() for l in lines], loc='upper right', frameon=False)\n",
    "    ax1.set_title(f'DQN Episode 1 — Reward vs Net Load — {bname}')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "    # 5) Reward vs Electricity Price\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 3.5))\n",
    "    ax1.plot(t, reward_ma, color='C3', label=f'Reward MA (w={window_reward})')\n",
    "    ax1.set_xlabel('Timestep'); ax1.set_ylabel(f'Reward MA ({window_reward})', color='C3')\n",
    "    ax1.tick_params(axis='y', labelcolor='C3'); ax1.grid(True, axis='x', alpha=0.3)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, price_ma, color='C0', label=f'Price MA (w={window_x})')\n",
    "    ax2.set_ylabel('Electricity Price', color='C0'); ax2.tick_params(axis='y', labelcolor='C0')\n",
    "    lines = ax1.get_lines() + ax2.get_lines()\n",
    "    ax1.legend(lines, [l.get_label() for l in lines], loc='upper right', frameon=False)\n",
    "    ax1.set_title(f'DQN Episode 1 — Reward vs Electricity Price — {bname}')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # 6) Reward vs Solar Generation\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 3.5))\n",
    "    ax1.plot(t, reward_ma, color='C3', label=f'Reward MA (w={window_reward})')\n",
    "    ax1.set_xlabel('Timestep'); ax1.set_ylabel(f'Reward MA ({window_reward})', color='C3')\n",
    "    ax1.tick_params(axis='y', labelcolor='C3'); ax1.grid(True, axis='x', alpha=0.3)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, solar_ma, color='C2', label=f'Solar Gen MA (w={window_x})')\n",
    "    ax2.set_ylabel('Solar Generation', color='C2'); ax2.tick_params(axis='y', labelcolor='C2')\n",
    "    lines = ax1.get_lines() + ax2.get_lines()\n",
    "    ax1.legend(lines, [l.get_label() for l in lines], loc='upper right', frameon=False)\n",
    "    ax1.set_title(f'DQN Episode 1 — Reward vs Solar Generation — {bname}')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_first_30_steps_actions_vs_loads_dqn(model, eval_env, building_name=\"Building_1\", deterministic=False)\n",
    "dqn_ep1_reward_vs_series(model, eval_env, building_name=\"Building_1\", deterministic=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (citylearn_env)",
   "language": "python",
   "name": "citylearn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
